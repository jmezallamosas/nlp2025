{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b4dd3e-f5e5-4671-a86a-82372ac1b0fc",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 2\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927209b-31e2-4f54-a95a-fbaf7e5f00a3",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd25ecf4-4b08-4d35-b150-5db899662272",
   "metadata": {},
   "source": [
    "Task 2: Training your own word embeddings (15 points)\n",
    "--------------------------------\n",
    "\n",
    "For this task, you'll use the `gensim` package to train your own embeddings for both words and characters. These will eventually act as inputs to your neural language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89edeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here are several dependencies to install\n",
    "# !python --version\n",
    "# !python -m pip install --upgrade pip setuptools wheel\n",
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# !pip install torch torchvision torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd110e8-9404-4bf3-b6c2-27120adfd79e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neurallm_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import your libraries here\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Remember to restart your kernel if you change the contents of this file!\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mneurallm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnutils\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# for word embeddings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# if not installed, run the following command:\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# !pip install gensim\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'neurallm_utils'"
     ]
    }
   ],
   "source": [
    "# import your libraries here\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "# for word embeddings\n",
    "# if not installed, run the following command:\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278aa8e0-2dce-4d33-8312-4b2b0146dcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running on google colab, you'll need to mount your drive to access data files\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce4aff-bba6-429d-b618-c57c23b350fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "\n",
    "# The dimensions of word embedding. \n",
    "# This variable will be used throughout the program\n",
    "# DO NOT WRITE \"50\" WHEN YOU ARE REFERRING TO THE EMBEDDING SIZE\n",
    "EMBEDDINGS_SIZE = 50\n",
    "\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bb948-4d52-41b4-a3e6-4b77d4232032",
   "metadata": {},
   "source": [
    "Train embeddings on provided dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d77d3-1d87-4cd4-a9b6-acbed5d2c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# use the provided utility functions to read in the data\n",
    "\n",
    "\n",
    "data = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "\n",
    "# read the spooky data in both by character and by word using the read_file_spooky function in the \n",
    "# provided utils\n",
    "\n",
    "\n",
    "# print out the first two sentences in each format\n",
    "# make sure we can read the output easily without scrolling to the side too much\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7704033c",
   "metadata": {},
   "source": [
    "8. What character represents spaces when we tokenize by character? __YOUR ANSWER HERE__\n",
    "9. Read the word2vec documentation. What do the following parameters signify?\n",
    "    - embeddings_size: __YOUR ANSWER HERE__\n",
    "    - window: __YOUR ANSWER HERE__\n",
    "    - min_count: __YOUR ANSWER HERE__\n",
    "    - sg: __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33920ce1-faee-4774-8aae-d78e511bf1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "# create your word embeddings\n",
    "# use the skip gram algorithm and a window size of 5\n",
    "# min_count should be 1\n",
    "# takes ~3.3 sec on Felix's computer for character embeddings using skip-gram with window size 5\n",
    "# takes ~3.3 sec on Felix's computer for word embeddings using skip-gram with window size 5 \n",
    "\n",
    "\n",
    "def train_word2vec(data: list[list[str]], embeddings_size: int,\n",
    "                    window: int = 5, min_count: int = 1, sg: int = 1) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Create new word embeddings based on our data.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A gensim Word2Vec model\n",
    "        https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# After you are happy with this function, copy + paste it into the bottom of \n",
    "# your neurallm_utils.py file\n",
    "# You'll need it for the next task!\n",
    "def create_embedder(raw_embeddings: Word2Vec) -> torch.nn.Embedding:\n",
    "    \"\"\"\n",
    "    Create a PyTorch embedding layer based on our data.\n",
    "\n",
    "    We will *first* train a Word2Vec model on our data.\n",
    "    Then, we'll use these weights to create a PyTorch embedding layer.\n",
    "        `nn.Embedding.from_pretrained(weights)`\n",
    "\n",
    "\n",
    "    PyTorch docs: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained\n",
    "    Gensim Word2Vec docs: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "    Pay particular attention to the *types* of the weights and the types required by PyTorch.\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embeddings_size: The dimensions in each embedding\n",
    "\n",
    "    Returns:\n",
    "        A PyTorch embedding layer\n",
    "    \"\"\"\n",
    "\n",
    "    # Hint:\n",
    "    # For later tasks, we'll need two mappings: One from token to index, and one from index to tokens.\n",
    "    # It might be a good idea to store these as properties of your embedder.\n",
    "    # e.g. `embedder.token_to_index = ...`\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db8e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and save both sets (word and character based) of Word2Vec embeddings. \n",
    "# Use the provided utility functions in nutils.\n",
    "# These will be (re)loaded in the next notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load them in again to make sure that this works and is still fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57301fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the embedders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d7c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at your saved token to index and index to token mappings in your embedders to make sure they make sense\n",
    "# AND that they are both dictionaries mapping from int to str or vice versa!\n",
    "# don't leave a ton of output in your notebook when you turn it in, but you need to understand this,\n",
    "# and it's an easy place to make a mistake that's hard to debug later.\n",
    "# do leave whatever code you use here, comment it out if it produces a lot of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66cea2-746e-4371-8966-33dd358d46c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
