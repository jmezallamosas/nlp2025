{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125534/2530367474.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/mezallamosas.j/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/mezallamosas.j/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "import neurallm_utils as nutils\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Data processing functions\n",
    "# -------------------------------\n",
    "\n",
    "def encode_tokens(data: list[list[str]], embedder: torch.nn.Embedding) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    finalList = []\n",
    "    for list in data:\n",
    "        currList = []\n",
    "        for word in list:\n",
    "            index = embedder.token_to_index[word]\n",
    "            currList.append(index)\n",
    "        finalList.append(currList)\n",
    "\n",
    "    return finalList\n",
    "\n",
    "\n",
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "      tokens (list): a list of tokens as strings\n",
    "      n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "      list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    # STUDENTS IMPLEMENT\n",
    "    res = []\n",
    "    for i in range(0, len(tokens)-n):\n",
    "        #append n gram + yth value\n",
    "        res.append(tokens[i:i+n+1])\n",
    "    return res\n",
    "\n",
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "\n",
    "    #1 2 3 4\n",
    "    #[1,2, y=3]\n",
    "    #[2,3, y=4]\n",
    "\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "    final_list = []\n",
    "    for list in encoded:\n",
    "        currList = create_ngrams(list, ngram-1)\n",
    "        final_list.extend(currList)\n",
    "    return final_list\n",
    "\n",
    "def split_sequences(training_sample):\n",
    "    x_sample = []\n",
    "    y_sample = []\n",
    "    for line in training_sample:\n",
    "        x_sample.append(line[0:-1])\n",
    "        y_sample.append(line[-1])\n",
    "    return x_sample, y_sample\n",
    "\n",
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataSet = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    test_size = int(len(dataSet)*test_pct)\n",
    "    train_size = len(dataSet) - test_size\n",
    "    train_data, test_data = torch.utils.data.random_split(dataSet, [train_size, test_size])\n",
    "    dataloader_train = DataLoader(train_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    dataloader_test = DataLoader(test_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    return dataloader_train, dataloader_test\n",
    "\n",
    "# -------------------------------\n",
    "# FFNN Model and Training Functions\n",
    "# -------------------------------\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Feed-Forward Neural Network for language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: Number of words in the vocabulary.\n",
    "            ngram: The N value (window size) for training.\n",
    "            embedding_layer: Pre-trained embedding layer.\n",
    "            hidden_units: Number of hidden units in the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram = ngram\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_units = hidden_units\n",
    "        self.device = device\n",
    "        \n",
    "        # Get embedding dimension from the provided embedder.\n",
    "        embedding_size = embedding_layer.embedding_dim\n",
    "        \n",
    "        # Define the network: flatten embedded n-gram tokens, then two linear layers with ReLU.\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=(ngram-1) * embedding_size, out_features=hidden_units, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=vocab_size, bias=True)\n",
    "        )\n",
    "        \n",
    "        # Move class to its own device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Params:\n",
    "            X: Tensor of input indices with shape (batch_size, ngram-1)\n",
    "        \n",
    "        Returns:\n",
    "            Logits of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding_layer(X)\n",
    "        flat_embedded = self.flatten(embedded)\n",
    "        logits = self.linear_relu_stack(flat_embedded)\n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "        optimizer.zero_grad()                  # Zero gradients for this batch.\n",
    "        outputs = model(inputs)                # Forward pass.\n",
    "        batch_loss = loss_fn(outputs, labels)  # Compute loss.\n",
    "        batch_loss.backward()                  # Backpropagation.\n",
    "        optimizer.step()                       # Update weights.\n",
    "        epoch_loss += batch_loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Params:\n",
    "        dataloader: Training data loader.\n",
    "        model: The model to train.\n",
    "        epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "    model.train()  # Set the model to training mode.\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        epoch_loss = train_one_epoch(dataloader, model, optimizer, loss_fn)\n",
    "        avg_epoch_loss = epoch_loss / n_batches\n",
    "        print(f\"Epoch: {epoch+1}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\"epoch\": epoch+1, \"avg_epoch_loss\": avg_epoch_loss})\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def full_pipeline(data, word_embeddings_filename: str, \n",
    "                  batch_size: int,\n",
    "                  ngram: int,\n",
    "                  hidden_units: int = 128,\n",
    "                  epochs: int = 1,\n",
    "                  lr: float = 0.001,\n",
    "                  test_pct: float = 0.1, device: str = \"cpu\") -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the full training pipeline from loading embeddings to model training.\n",
    "    \n",
    "    Params:\n",
    "        data: Raw data as a list of lists of tokens (here, integer indices).\n",
    "        word_embeddings_filename: Filename for the pre-trained embeddings.\n",
    "        batch_size: Batch size for training.\n",
    "        ngram: N-gram size.\n",
    "        hidden_units: Number of hidden units.\n",
    "        epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "        test_pct: Percentage of data for testing (not used in training).\n",
    "    \n",
    "    Returns:\n",
    "        The trained FFNN model.\n",
    "    \"\"\"\n",
    "    # Load embeddings and create an embedder.\n",
    "    token_embeddings = nutils.load_word2vec(word_embeddings_filename)\n",
    "    embedder = nutils.create_embedder(token_embeddings)\n",
    "    \n",
    "    # Preprocess data.\n",
    "    encoded_tokens = encode_tokens(data, embedder)\n",
    "    vocab_size = embedder.num_embeddings\n",
    "    training_sample = generate_ngram_training_samples(encoded_tokens, ngram)\n",
    "    x_sample, y_sample = split_sequences(training_sample)\n",
    "    dataloader_train, _ = create_dataloaders(x_sample, y_sample, batch_size, test_pct)\n",
    "    \n",
    "    # Initialize the model.\n",
    "    model = FFNN(vocab_size=vocab_size, ngram=ngram, embedding_layer=embedder, hidden_units=hidden_units, device=device)\n",
    "\n",
    "    # Train the model.\n",
    "    final_loss = train(dataloader=dataloader_train, model=model, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return model, final_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction and generation functions\n",
    "# -------------------------------\n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\t# Encode tokens\n",
    "    encoded_tokens = [model.embedding_layer.token_to_index[token] for token in input_tokens]\n",
    "    \n",
    "\t# Trasform to tensor\n",
    "    encoded_tokens = torch.tensor([encoded_tokens]).to(model.device) # Dim [1, ngram-1]\n",
    "    \n",
    "    # Setting model to evaluation mode turns off Dropout and BatchNorm making the predictions deterministic\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    \n",
    "    with torch.no_grad(): # Speeds up inference and reduces memory usage by not having to calcualte gradients\n",
    "        logits = model(encoded_tokens) # Forward pass on the model\n",
    "        probability = nn.functional.softmax(logits, dim=1) # Normalize z scores to probability\n",
    "        predicted_idx = torch.multinomial(probability, num_samples=1).item()\n",
    "\n",
    "        #predicted_idx = probability.argmax(dim=1).item() # Retrieve int value\n",
    "\t\t\n",
    "\t# Transform index to natural-language token\n",
    "    predicted_token = model.embedding_layer.index_to_token[predicted_idx] \n",
    "    \n",
    "    return predicted_token\n",
    "\n",
    "from typing import List\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: List[str], max_tokens: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\" \n",
    "    n_tokens = 0 # Count tokens that have been generated\n",
    "    tokens = seed.copy() # Copy of initial seed\n",
    "    end_token = \"<\\s>\"\n",
    "    \n",
    "    while True:\n",
    "        for_prediction = seed[-(model.ngram-1):]\n",
    "        predicted_token = predict(model, for_prediction)\n",
    "        if predicted_token == end_token:\n",
    "        \tbreak\n",
    "        tokens.append(predicted_token)\n",
    "        n_tokens += 1\n",
    "        if max_tokens is not None and n_tokens >= max_tokens:\n",
    "            break\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def generate_sentences(model, seed: List[str],  n_sentences: int, max_tokens: int = None) -> List[str]:\n",
    "    return [generate(model, seed, max_tokens) for i in range(n_sentences)]\n",
    "\n",
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences\n",
    "\n",
    "def format_sentence(tokens_list: List[List[str]], by_char = False) -> str:\n",
    "  \"\"\"Removes <s> at the start of the sentence and </s> at ehe end. Joins the list of tokens into a string and capitalizes it.\n",
    "  Args:\n",
    "    tokens (list(list)): the list of tokens list to be formatted into a sentence\n",
    "\n",
    "  Returns:\n",
    "    string: formatted sentence as a string\n",
    "  \n",
    "  \"\"\"\n",
    "  text = \"\" # Initializing final sentence\n",
    "  for tokens in tokens_list: # Parsing through each individual sentence\n",
    "    while tokens[0] == '<s>': # Removes all <s> at the beggining even if there are several for ngram > 2 models\n",
    "      tokens.pop(0)\n",
    "    if tokens[-1] == '</s>': # Removes the one </s> at the end of the sentence\n",
    "      tokens.pop(-1)\n",
    "    if by_char:\n",
    "      sentence = \"\".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    else:\n",
    "      sentence = \" \".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    text += sentence + \".\\n\" # Adds a period and space separator between sentences\n",
    "  return text.strip(\" \") # Removes the last space in the last sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t# Initialize a wandb run (hyperparameters come from wandb.config).\n",
    "\twandb.init(\n",
    "\t\tentity = \"northeastern-university\",\n",
    "\t\tproject = \"neural-language-model\"\n",
    "\t\t)\n",
    "\t\n",
    "\tconfig = wandb.config\n",
    "\n",
    "\tEMBEDDINGS_SIZE = config.embeddings_size\n",
    "\tNGRAM = config.ngram\n",
    "\tNUM_SEQUENCES_PER_BATCH = config.batch_size\n",
    "\tHIDDEN_UNITS = config.hidden_units\n",
    "\tEPOCHS = config.epochs\n",
    "\tLR = config.lr\n",
    "\tTEXT_TYPE = \"word\"\n",
    "\n",
    "\tTRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "\tEMBEDDING_SAVE_FILE_WORD = f\"embeddings/spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "\tEMBEDDING_SAVE_FILE_CHAR = f\"embeddings/spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "\tMODEL_FILE_WORD = f'models/spooky_author_model_word_{EMBEDDINGS_SIZE}_{NUM_SEQUENCES_PER_BATCH}_{NGRAM}_{HIDDEN_UNITS}_{EPOCHS}_{LR}.pt' # The file to save your trained word-based neural LM to\n",
    "\tMODEL_FILE_CHAR = f'models/spooky_author_model_char_{EMBEDDINGS_SIZE}_{NUM_SEQUENCES_PER_BATCH}_{NGRAM}_{HIDDEN_UNITS}_{EPOCHS}_{LR}.pt' # The file to save your trained char-based neural LM to\n",
    "\n",
    "\tif TEXT_TYPE == \"word\":\n",
    "\t\tdata = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\t\tword_embeddings_filename = EMBEDDING_SAVE_FILE_WORD\n",
    "\t\tif not os.path.exists(word_embeddings_filename):\n",
    "\t\t\ttrained_word = nutils.train_word2vec(data, EMBEDDINGS_SIZE)\n",
    "\t\t\tnutils.save_word2vec(trained_word, EMBEDDING_SAVE_FILE_WORD)\n",
    "\n",
    "\telif TEXT_TYPE == \"char\":\n",
    "\t\tdata = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "\t\tword_embeddings_filename = EMBEDDING_SAVE_FILE_CHAR\n",
    "\t\tif not os.path.exists(word_embeddings_filename):\n",
    "\t\t\ttrained_char = nutils.train_word2vec(data, EMBEDDINGS_SIZE)\n",
    "\t\t\tnutils.save_word2vec(trained_char, EMBEDDING_SAVE_FILE_CHAR)\n",
    "\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\tmodel, final_loss = full_pipeline(\n",
    "\t\tdata=data,\n",
    "\t\tword_embeddings_filename = word_embeddings_filename,\n",
    "\t\tbatch_size=NUM_SEQUENCES_PER_BATCH,\n",
    "\t\tngram=NGRAM,\n",
    "\t\thidden_units=config.hidden_units,\n",
    "\t\tepochs=config.epochs,\n",
    "\t\tlr=config.lr,\n",
    "\t\ttest_pct=config.test_pct,\n",
    "\t\tdevice = device\n",
    "\t)\n",
    "\n",
    "\tif TEXT_TYPE == \"word\":\n",
    "\t\ttorch.save(model.state_dict(), MODEL_FILE_WORD)\n",
    "\n",
    "\telif TEXT_TYPE == \"char\":\n",
    "\t\ttorch.save(model.state_dict(), MODEL_FILE_CHAR)\n",
    "\n",
    "\twandb.log({\"final_loss\": final_loss})\n",
    "\twandb.finish()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    \"name\": \"word_bayes_hyperparameter_sweep\",\n",
    "\t\"method\": \"bayes\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "\t\"metric\": {\n",
    "\t\t\"name\": \"avg_epoch_loss\",\n",
    "\t\t\"goal\": \"minimize\"  # We want to minimize the training loss.\n",
    "\t},\n",
    "\t\"parameters\": {\n",
    "\t\t\"embeddings_size\": {\"values\": [50, 100, 200]},\n",
    "\t\t\"batch_size\": {\"values\": [128, 256, 512]},\n",
    "\t\t\"ngram\": {\"values\": [2, 3, 4, 5]},\n",
    "\t\t\"hidden_units\": {\"values\": [128, 256, 512]},\n",
    "\t\t\"epochs\": {\"values\": [5, 10, 25, 50]},\n",
    "\t\t\"lr\": {\"values\": [0.001, 0.0001, 0.00001]},\n",
    "\t\t\"test_pct\": {\"value\": 0.1} # Fixed value.\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 2g0npqmk\n",
      "Sweep URL: https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk\n",
      "Sweep ID: 2g0npqmk\n"
     ]
    }
   ],
   "source": [
    "# Register the sweep with wandb.\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       project=\"neural-language-model\")\n",
    "print(\"Sweep ID:\", sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4k50rzs1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmezallamosasj\u001b[0m (\u001b[33mbiofx\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_215809-4k50rzs1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/4k50rzs1' target=\"_blank\">decent-sweep-1</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/4k50rzs1' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/4k50rzs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd0eea0556b49f78accdf1402d78089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 5.9698\n",
      "Epoch: 2, Average Loss: 5.4900\n",
      "Epoch: 3, Average Loss: 5.3237\n",
      "Epoch: 4, Average Loss: 5.2172\n",
      "Epoch: 5, Average Loss: 5.1430\n",
      "Epoch: 6, Average Loss: 5.0893\n",
      "Epoch: 7, Average Loss: 5.0483\n",
      "Epoch: 8, Average Loss: 5.0162\n",
      "Epoch: 9, Average Loss: 4.9908\n",
      "Epoch: 10, Average Loss: 4.9697\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.96974</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>final_loss</td><td>4.96974</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-1</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/4k50rzs1' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/4k50rzs1</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_215809-4k50rzs1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qouoxs03 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_220020-qouoxs03</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/qouoxs03' target=\"_blank\">wandering-sweep-2</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/qouoxs03' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/qouoxs03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55958595c5df40a99aa12f07ac54bbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 6.5732\n",
      "Epoch: 2, Average Loss: 5.7254\n",
      "Epoch: 3, Average Loss: 5.5987\n",
      "Epoch: 4, Average Loss: 5.5057\n",
      "Epoch: 5, Average Loss: 5.4277\n",
      "Epoch: 6, Average Loss: 5.3600\n",
      "Epoch: 7, Average Loss: 5.3010\n",
      "Epoch: 8, Average Loss: 5.2493\n",
      "Epoch: 9, Average Loss: 5.2037\n",
      "Epoch: 10, Average Loss: 5.1626\n",
      "Epoch: 11, Average Loss: 5.1256\n",
      "Epoch: 12, Average Loss: 5.0915\n",
      "Epoch: 13, Average Loss: 5.0599\n",
      "Epoch: 14, Average Loss: 5.0302\n",
      "Epoch: 15, Average Loss: 5.0023\n",
      "Epoch: 16, Average Loss: 4.9757\n",
      "Epoch: 17, Average Loss: 4.9504\n",
      "Epoch: 18, Average Loss: 4.9263\n",
      "Epoch: 19, Average Loss: 4.9031\n",
      "Epoch: 20, Average Loss: 4.8806\n",
      "Epoch: 21, Average Loss: 4.8590\n",
      "Epoch: 22, Average Loss: 4.8381\n",
      "Epoch: 23, Average Loss: 4.8178\n",
      "Epoch: 24, Average Loss: 4.7980\n",
      "Epoch: 25, Average Loss: 4.7788\n",
      "Epoch: 26, Average Loss: 4.7599\n",
      "Epoch: 27, Average Loss: 4.7415\n",
      "Epoch: 28, Average Loss: 4.7234\n",
      "Epoch: 29, Average Loss: 4.7056\n",
      "Epoch: 30, Average Loss: 4.6882\n",
      "Epoch: 31, Average Loss: 4.6710\n",
      "Epoch: 32, Average Loss: 4.6542\n",
      "Epoch: 33, Average Loss: 4.6375\n",
      "Epoch: 34, Average Loss: 4.6211\n",
      "Epoch: 35, Average Loss: 4.6049\n",
      "Epoch: 36, Average Loss: 4.5889\n",
      "Epoch: 37, Average Loss: 4.5731\n",
      "Epoch: 38, Average Loss: 4.5574\n",
      "Epoch: 39, Average Loss: 4.5420\n",
      "Epoch: 40, Average Loss: 4.5267\n",
      "Epoch: 41, Average Loss: 4.5118\n",
      "Epoch: 42, Average Loss: 4.4969\n",
      "Epoch: 43, Average Loss: 4.4823\n",
      "Epoch: 44, Average Loss: 4.4679\n",
      "Epoch: 45, Average Loss: 4.4537\n",
      "Epoch: 46, Average Loss: 4.4396\n",
      "Epoch: 47, Average Loss: 4.4258\n",
      "Epoch: 48, Average Loss: 4.4123\n",
      "Epoch: 49, Average Loss: 4.3990\n",
      "Epoch: 50, Average Loss: 4.3860\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▅▅▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.38601</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>4.38601</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-sweep-2</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/qouoxs03' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/qouoxs03</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_220020-qouoxs03/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pjgb4c1f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_221405-pjgb4c1f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/pjgb4c1f' target=\"_blank\">divine-sweep-3</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/pjgb4c1f' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/pjgb4c1f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3aea6fd539e46a3ad46a5ba79063690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 6.3058\n",
      "Epoch: 2, Average Loss: 5.5610\n",
      "Epoch: 3, Average Loss: 5.3872\n",
      "Epoch: 4, Average Loss: 5.2710\n",
      "Epoch: 5, Average Loss: 5.1875\n",
      "Epoch: 6, Average Loss: 5.1215\n",
      "Epoch: 7, Average Loss: 5.0663\n",
      "Epoch: 8, Average Loss: 5.0171\n",
      "Epoch: 9, Average Loss: 4.9728\n",
      "Epoch: 10, Average Loss: 4.9318\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▄▃▃▂▂▂▁▁▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.93182</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>final_loss</td><td>4.93182</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">divine-sweep-3</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/pjgb4c1f' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/pjgb4c1f</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_221405-pjgb4c1f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q05p3cum with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_221526-q05p3cum</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/q05p3cum' target=\"_blank\">faithful-sweep-4</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/q05p3cum' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/q05p3cum</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2d0f71a9df40e78baa93626fd4429c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 6.5617\n",
      "Epoch: 2, Average Loss: 5.7268\n",
      "Epoch: 3, Average Loss: 5.6029\n",
      "Epoch: 4, Average Loss: 5.5125\n",
      "Epoch: 5, Average Loss: 5.4366\n",
      "Epoch: 6, Average Loss: 5.3703\n",
      "Epoch: 7, Average Loss: 5.3119\n",
      "Epoch: 8, Average Loss: 5.2600\n",
      "Epoch: 9, Average Loss: 5.2138\n",
      "Epoch: 10, Average Loss: 5.1721\n",
      "Epoch: 11, Average Loss: 5.1343\n",
      "Epoch: 12, Average Loss: 5.0996\n",
      "Epoch: 13, Average Loss: 5.0674\n",
      "Epoch: 14, Average Loss: 5.0373\n",
      "Epoch: 15, Average Loss: 5.0091\n",
      "Epoch: 16, Average Loss: 4.9822\n",
      "Epoch: 17, Average Loss: 4.9567\n",
      "Epoch: 18, Average Loss: 4.9325\n",
      "Epoch: 19, Average Loss: 4.9091\n",
      "Epoch: 20, Average Loss: 4.8866\n",
      "Epoch: 21, Average Loss: 4.8650\n",
      "Epoch: 22, Average Loss: 4.8439\n",
      "Epoch: 23, Average Loss: 4.8236\n",
      "Epoch: 24, Average Loss: 4.8039\n",
      "Epoch: 25, Average Loss: 4.7847\n",
      "Epoch: 26, Average Loss: 4.7659\n",
      "Epoch: 27, Average Loss: 4.7475\n",
      "Epoch: 28, Average Loss: 4.7294\n",
      "Epoch: 29, Average Loss: 4.7118\n",
      "Epoch: 30, Average Loss: 4.6944\n",
      "Epoch: 31, Average Loss: 4.6773\n",
      "Epoch: 32, Average Loss: 4.6604\n",
      "Epoch: 33, Average Loss: 4.6439\n",
      "Epoch: 34, Average Loss: 4.6276\n",
      "Epoch: 35, Average Loss: 4.6114\n",
      "Epoch: 36, Average Loss: 4.5955\n",
      "Epoch: 37, Average Loss: 4.5796\n",
      "Epoch: 38, Average Loss: 4.5641\n",
      "Epoch: 39, Average Loss: 4.5487\n",
      "Epoch: 40, Average Loss: 4.5336\n",
      "Epoch: 41, Average Loss: 4.5186\n",
      "Epoch: 42, Average Loss: 4.5039\n",
      "Epoch: 43, Average Loss: 4.4893\n",
      "Epoch: 44, Average Loss: 4.4749\n",
      "Epoch: 45, Average Loss: 4.4607\n",
      "Epoch: 46, Average Loss: 4.4468\n",
      "Epoch: 47, Average Loss: 4.4331\n",
      "Epoch: 48, Average Loss: 4.4195\n",
      "Epoch: 49, Average Loss: 4.4063\n",
      "Epoch: 50, Average Loss: 4.3933\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.39328</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>4.39328</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">faithful-sweep-4</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/q05p3cum' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/q05p3cum</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_221526-q05p3cum/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tshujjmw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_222904-tshujjmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/tshujjmw' target=\"_blank\">smooth-sweep-5</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/tshujjmw' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/tshujjmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8279620bef53498e9f396dfddd15f023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 6.5859\n",
      "Epoch: 2, Average Loss: 5.7304\n",
      "Epoch: 3, Average Loss: 5.6033\n",
      "Epoch: 4, Average Loss: 5.5110\n",
      "Epoch: 5, Average Loss: 5.4337\n",
      "Epoch: 6, Average Loss: 5.3665\n",
      "Epoch: 7, Average Loss: 5.3079\n",
      "Epoch: 8, Average Loss: 5.2560\n",
      "Epoch: 9, Average Loss: 5.2100\n",
      "Epoch: 10, Average Loss: 5.1688\n",
      "Epoch: 11, Average Loss: 5.1314\n",
      "Epoch: 12, Average Loss: 5.0970\n",
      "Epoch: 13, Average Loss: 5.0651\n",
      "Epoch: 14, Average Loss: 5.0353\n",
      "Epoch: 15, Average Loss: 5.0072\n",
      "Epoch: 16, Average Loss: 4.9806\n",
      "Epoch: 17, Average Loss: 4.9552\n",
      "Epoch: 18, Average Loss: 4.9309\n",
      "Epoch: 19, Average Loss: 4.9077\n",
      "Epoch: 20, Average Loss: 4.8854\n",
      "Epoch: 21, Average Loss: 4.8637\n",
      "Epoch: 22, Average Loss: 4.8429\n",
      "Epoch: 23, Average Loss: 4.8226\n",
      "Epoch: 24, Average Loss: 4.8029\n",
      "Epoch: 25, Average Loss: 4.7837\n",
      "Epoch: 26, Average Loss: 4.7650\n",
      "Epoch: 27, Average Loss: 4.7466\n",
      "Epoch: 28, Average Loss: 4.7286\n",
      "Epoch: 29, Average Loss: 4.7110\n",
      "Epoch: 30, Average Loss: 4.6936\n",
      "Epoch: 31, Average Loss: 4.6765\n",
      "Epoch: 32, Average Loss: 4.6597\n",
      "Epoch: 33, Average Loss: 4.6432\n",
      "Epoch: 34, Average Loss: 4.6269\n",
      "Epoch: 35, Average Loss: 4.6107\n",
      "Epoch: 36, Average Loss: 4.5948\n",
      "Epoch: 37, Average Loss: 4.5791\n",
      "Epoch: 38, Average Loss: 4.5635\n",
      "Epoch: 39, Average Loss: 4.5481\n",
      "Epoch: 40, Average Loss: 4.5330\n",
      "Epoch: 41, Average Loss: 4.5180\n",
      "Epoch: 42, Average Loss: 4.5033\n",
      "Epoch: 43, Average Loss: 4.4887\n",
      "Epoch: 44, Average Loss: 4.4743\n",
      "Epoch: 45, Average Loss: 4.4601\n",
      "Epoch: 46, Average Loss: 4.4461\n",
      "Epoch: 47, Average Loss: 4.4323\n",
      "Epoch: 48, Average Loss: 4.4188\n",
      "Epoch: 49, Average Loss: 4.4055\n",
      "Epoch: 50, Average Loss: 4.3924\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.39243</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>4.39243</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-5</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/tshujjmw' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/tshujjmw</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_222904-tshujjmw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o1nrkbjg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_224241-o1nrkbjg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/o1nrkbjg' target=\"_blank\">young-sweep-6</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/o1nrkbjg' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/o1nrkbjg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6482c833b7f9477f943a9d10e18eed07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 6.7483\n",
      "Epoch: 2, Average Loss: 5.7691\n",
      "Epoch: 3, Average Loss: 5.6363\n",
      "Epoch: 4, Average Loss: 5.5433\n",
      "Epoch: 5, Average Loss: 5.4683\n",
      "Epoch: 6, Average Loss: 5.4041\n",
      "Epoch: 7, Average Loss: 5.3479\n",
      "Epoch: 8, Average Loss: 5.2977\n",
      "Epoch: 9, Average Loss: 5.2525\n",
      "Epoch: 10, Average Loss: 5.2114\n",
      "Epoch: 11, Average Loss: 5.1739\n",
      "Epoch: 12, Average Loss: 5.1392\n",
      "Epoch: 13, Average Loss: 5.1069\n",
      "Epoch: 14, Average Loss: 5.0767\n",
      "Epoch: 15, Average Loss: 5.0483\n",
      "Epoch: 16, Average Loss: 5.0213\n",
      "Epoch: 17, Average Loss: 4.9955\n",
      "Epoch: 18, Average Loss: 4.9709\n",
      "Epoch: 19, Average Loss: 4.9473\n",
      "Epoch: 20, Average Loss: 4.9246\n",
      "Epoch: 21, Average Loss: 4.9026\n",
      "Epoch: 22, Average Loss: 4.8815\n",
      "Epoch: 23, Average Loss: 4.8608\n",
      "Epoch: 24, Average Loss: 4.8408\n",
      "Epoch: 25, Average Loss: 4.8213\n",
      "Epoch: 26, Average Loss: 4.8024\n",
      "Epoch: 27, Average Loss: 4.7838\n",
      "Epoch: 28, Average Loss: 4.7656\n",
      "Epoch: 29, Average Loss: 4.7478\n",
      "Epoch: 30, Average Loss: 4.7303\n",
      "Epoch: 31, Average Loss: 4.7132\n",
      "Epoch: 32, Average Loss: 4.6963\n",
      "Epoch: 33, Average Loss: 4.6797\n",
      "Epoch: 34, Average Loss: 4.6632\n",
      "Epoch: 35, Average Loss: 4.6471\n",
      "Epoch: 36, Average Loss: 4.6312\n",
      "Epoch: 37, Average Loss: 4.6156\n",
      "Epoch: 38, Average Loss: 4.6001\n",
      "Epoch: 39, Average Loss: 4.5849\n",
      "Epoch: 40, Average Loss: 4.5698\n",
      "Epoch: 41, Average Loss: 4.5550\n",
      "Epoch: 42, Average Loss: 4.5404\n",
      "Epoch: 43, Average Loss: 4.5260\n",
      "Epoch: 44, Average Loss: 4.5120\n",
      "Epoch: 45, Average Loss: 4.4981\n",
      "Epoch: 46, Average Loss: 4.4845\n",
      "Epoch: 47, Average Loss: 4.4710\n",
      "Epoch: 48, Average Loss: 4.4579\n",
      "Epoch: 49, Average Loss: 4.4450\n",
      "Epoch: 50, Average Loss: 4.4324\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>4.43236</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>4.43236</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-sweep-6</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/o1nrkbjg' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/o1nrkbjg</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_224241-o1nrkbjg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: et4ls8u6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_225623-et4ls8u6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/et4ls8u6' target=\"_blank\">genial-sweep-7</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/et4ls8u6' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/et4ls8u6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f001ec327a43ffb42efb87c2daab27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 5.3684\n",
      "Epoch: 2, Average Loss: 4.7953\n",
      "Epoch: 3, Average Loss: 4.4893\n",
      "Epoch: 4, Average Loss: 4.2335\n",
      "Epoch: 5, Average Loss: 4.0110\n",
      "Epoch: 6, Average Loss: 3.8206\n",
      "Epoch: 7, Average Loss: 3.6682\n",
      "Epoch: 8, Average Loss: 3.5512\n",
      "Epoch: 9, Average Loss: 3.4632\n",
      "Epoch: 10, Average Loss: 3.3947\n",
      "Epoch: 11, Average Loss: 3.3384\n",
      "Epoch: 12, Average Loss: 3.2922\n",
      "Epoch: 13, Average Loss: 3.2530\n",
      "Epoch: 14, Average Loss: 3.2186\n",
      "Epoch: 15, Average Loss: 3.1883\n",
      "Epoch: 16, Average Loss: 3.1613\n",
      "Epoch: 17, Average Loss: 3.1378\n",
      "Epoch: 18, Average Loss: 3.1161\n",
      "Epoch: 19, Average Loss: 3.0965\n",
      "Epoch: 20, Average Loss: 3.0788\n",
      "Epoch: 21, Average Loss: 3.0621\n",
      "Epoch: 22, Average Loss: 3.0472\n",
      "Epoch: 23, Average Loss: 3.0341\n",
      "Epoch: 24, Average Loss: 3.0212\n",
      "Epoch: 25, Average Loss: 3.0089\n",
      "Epoch: 26, Average Loss: 2.9987\n",
      "Epoch: 27, Average Loss: 2.9880\n",
      "Epoch: 28, Average Loss: 2.9783\n",
      "Epoch: 29, Average Loss: 2.9692\n",
      "Epoch: 30, Average Loss: 2.9610\n",
      "Epoch: 31, Average Loss: 2.9523\n",
      "Epoch: 32, Average Loss: 2.9453\n",
      "Epoch: 33, Average Loss: 2.9376\n",
      "Epoch: 34, Average Loss: 2.9313\n",
      "Epoch: 35, Average Loss: 2.9245\n",
      "Epoch: 36, Average Loss: 2.9183\n",
      "Epoch: 37, Average Loss: 2.9135\n",
      "Epoch: 38, Average Loss: 2.9077\n",
      "Epoch: 39, Average Loss: 2.9018\n",
      "Epoch: 40, Average Loss: 2.8968\n",
      "Epoch: 41, Average Loss: 2.8919\n",
      "Epoch: 42, Average Loss: 2.8875\n",
      "Epoch: 43, Average Loss: 2.8827\n",
      "Epoch: 44, Average Loss: 2.8780\n",
      "Epoch: 45, Average Loss: 2.8755\n",
      "Epoch: 46, Average Loss: 2.8707\n",
      "Epoch: 47, Average Loss: 2.8661\n",
      "Epoch: 48, Average Loss: 2.8633\n",
      "Epoch: 49, Average Loss: 2.8598\n",
      "Epoch: 50, Average Loss: 2.8558\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▆▆▅▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>2.85578</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>2.85578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">genial-sweep-7</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/et4ls8u6' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/et4ls8u6</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_225623-et4ls8u6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ccj03av1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_231005-ccj03av1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/ccj03av1' target=\"_blank\">wise-sweep-8</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/ccj03av1' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/ccj03av1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8841f032bf16499389555a3594ef24c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 5.3679\n",
      "Epoch: 2, Average Loss: 4.7962\n",
      "Epoch: 3, Average Loss: 4.4922\n",
      "Epoch: 4, Average Loss: 4.2329\n",
      "Epoch: 5, Average Loss: 4.0077\n",
      "Epoch: 6, Average Loss: 3.8151\n",
      "Epoch: 7, Average Loss: 3.6613\n",
      "Epoch: 8, Average Loss: 3.5467\n",
      "Epoch: 9, Average Loss: 3.4603\n",
      "Epoch: 10, Average Loss: 3.3937\n",
      "Epoch: 11, Average Loss: 3.3386\n",
      "Epoch: 12, Average Loss: 3.2924\n",
      "Epoch: 13, Average Loss: 3.2527\n",
      "Epoch: 14, Average Loss: 3.2189\n",
      "Epoch: 15, Average Loss: 3.1889\n",
      "Epoch: 16, Average Loss: 3.1630\n",
      "Epoch: 17, Average Loss: 3.1397\n",
      "Epoch: 18, Average Loss: 3.1179\n",
      "Epoch: 19, Average Loss: 3.0979\n",
      "Epoch: 20, Average Loss: 3.0809\n",
      "Epoch: 21, Average Loss: 3.0644\n",
      "Epoch: 22, Average Loss: 3.0498\n",
      "Epoch: 23, Average Loss: 3.0359\n",
      "Epoch: 24, Average Loss: 3.0229\n",
      "Epoch: 25, Average Loss: 3.0112\n",
      "Epoch: 26, Average Loss: 2.9995\n",
      "Epoch: 27, Average Loss: 2.9900\n",
      "Epoch: 28, Average Loss: 2.9800\n",
      "Epoch: 29, Average Loss: 2.9712\n",
      "Epoch: 30, Average Loss: 2.9620\n",
      "Epoch: 31, Average Loss: 2.9539\n",
      "Epoch: 32, Average Loss: 2.9470\n",
      "Epoch: 33, Average Loss: 2.9394\n",
      "Epoch: 34, Average Loss: 2.9324\n",
      "Epoch: 35, Average Loss: 2.9257\n",
      "Epoch: 36, Average Loss: 2.9197\n",
      "Epoch: 37, Average Loss: 2.9131\n",
      "Epoch: 38, Average Loss: 2.9079\n",
      "Epoch: 39, Average Loss: 2.9027\n",
      "Epoch: 40, Average Loss: 2.8971\n",
      "Epoch: 41, Average Loss: 2.8925\n",
      "Epoch: 42, Average Loss: 2.8874\n",
      "Epoch: 43, Average Loss: 2.8830\n",
      "Epoch: 44, Average Loss: 2.8782\n",
      "Epoch: 45, Average Loss: 2.8744\n",
      "Epoch: 46, Average Loss: 2.8700\n",
      "Epoch: 47, Average Loss: 2.8666\n",
      "Epoch: 48, Average Loss: 2.8632\n",
      "Epoch: 49, Average Loss: 2.8594\n",
      "Epoch: 50, Average Loss: 2.8562\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>█▆▆▅▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>final_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_epoch_loss</td><td>2.85618</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>final_loss</td><td>2.85618</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-sweep-8</strong> at: <a href='https://wandb.ai/biofx/neural-language-model/runs/ccj03av1' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/ccj03av1</a><br> View project at: <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_231005-ccj03av1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ww94o91x with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_units: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tngram: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_pct: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'neural-language-model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'northeastern-university' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mezallamosas.j/nlp2025/wandb/run-20250311_232353-ww94o91x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/biofx/neural-language-model/runs/ww94o91x' target=\"_blank\">peachy-sweep-9</a></strong> to <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/biofx/neural-language-model' target=\"_blank\">https://wandb.ai/biofx/neural-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/sweeps/2g0npqmk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/biofx/neural-language-model/runs/ww94o91x' target=\"_blank\">https://wandb.ai/biofx/neural-language-model/runs/ww94o91x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05484f85b424906bb034dfe775de2cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Average Loss: 5.3666\n",
      "Epoch: 2, Average Loss: 4.7724\n",
      "Epoch: 3, Average Loss: 4.4369\n",
      "Epoch: 4, Average Loss: 4.1046\n",
      "Epoch: 5, Average Loss: 3.7914\n",
      "Epoch: 6, Average Loss: 3.5581\n",
      "Epoch: 7, Average Loss: 3.3963\n",
      "Epoch: 8, Average Loss: 3.2769\n"
     ]
    }
   ],
   "source": [
    "# Set count to the number of runs you wish to execute; here, 5 runs are used as an example.\n",
    "wandb.agent(sweep_id, function=main, count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
