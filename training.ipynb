{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mezallamosas.j\\AppData\\Local\\Temp\\ipykernel_18676\\3591027982.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mezallamosas.j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mezallamosas.j\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "import neurallm_utils as nutils\n",
    "import wandb\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Data processing functions\n",
    "# -------------------------------\n",
    "\n",
    "def encode_tokens(data: list[list[str]], embedder: torch.nn.Embedding) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    finalList = []\n",
    "    for list in data:\n",
    "        currList = []\n",
    "        for word in list:\n",
    "            index = embedder.token_to_index[word]\n",
    "            currList.append(index)\n",
    "        finalList.append(currList)\n",
    "\n",
    "    return finalList\n",
    "\n",
    "\n",
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "      tokens (list): a list of tokens as strings\n",
    "      n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "      list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    # STUDENTS IMPLEMENT\n",
    "    res = []\n",
    "    for i in range(0, len(tokens)-n):\n",
    "        #append n gram + yth value\n",
    "        res.append(tokens[i:i+n+1])\n",
    "    return res\n",
    "\n",
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "\n",
    "    #1 2 3 4\n",
    "    #[1,2, y=3]\n",
    "    #[2,3, y=4]\n",
    "\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "    final_list = []\n",
    "    for list in encoded:\n",
    "        currList = create_ngrams(list, ngram-1)\n",
    "        final_list.extend(currList)\n",
    "    return final_list\n",
    "\n",
    "def split_sequences(training_sample):\n",
    "    x_sample = []\n",
    "    y_sample = []\n",
    "    for line in training_sample:\n",
    "        x_sample.append(line[0:-1])\n",
    "        y_sample.append(line[-1])\n",
    "    return x_sample, y_sample\n",
    "\n",
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataSet = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    test_size = int(len(dataSet)*test_pct)\n",
    "    train_size = len(dataSet) - test_size\n",
    "    train_data, test_data = torch.utils.data.random_split(dataSet, [train_size, test_size])\n",
    "    dataloader_train = DataLoader(train_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    dataloader_test = DataLoader(test_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    return dataloader_train, dataloader_test\n",
    "\n",
    "# -------------------------------\n",
    "# FFNN Model and Training Functions\n",
    "# -------------------------------\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Feed-Forward Neural Network for language modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128, device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: Number of words in the vocabulary.\n",
    "            ngram: The N value (window size) for training.\n",
    "            embedding_layer: Pre-trained embedding layer.\n",
    "            hidden_units: Number of hidden units in the hidden layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram = ngram\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_units = hidden_units\n",
    "        self.device = device\n",
    "        \n",
    "        # Get embedding dimension from the provided embedder.\n",
    "        embedding_size = embedding_layer.embedding_dim\n",
    "        \n",
    "        # Define the network: flatten embedded n-gram tokens, then two linear layers with ReLU.\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=(ngram-1) * embedding_size, out_features=hidden_units, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_units, out_features=vocab_size, bias=True)\n",
    "        )\n",
    "        \n",
    "        # Move class to its own device\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Params:\n",
    "            X: Tensor of input indices with shape (batch_size, ngram-1)\n",
    "        \n",
    "        Returns:\n",
    "            Logits of shape (batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        embedded = self.embedding_layer(X)\n",
    "        flat_embedded = self.flatten(embedded)\n",
    "        logits = self.linear_relu_stack(flat_embedded)\n",
    "        return logits\n",
    "\n",
    "def train_one_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(model.device), labels.to(model.device)\n",
    "        optimizer.zero_grad()                  # Zero gradients for this batch.\n",
    "        outputs = model(inputs)                # Forward pass.\n",
    "        batch_loss = loss_fn(outputs, labels)  # Compute loss.\n",
    "        batch_loss.backward()                  # Backpropagation.\n",
    "        optimizer.step()                       # Update weights.\n",
    "        epoch_loss += batch_loss.item()\n",
    "    return epoch_loss\n",
    "\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "    \n",
    "    Params:\n",
    "        dataloader: Training data loader.\n",
    "        model: The model to train.\n",
    "        epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "    model.train()  # Set the model to training mode.\n",
    "    \n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n",
    "        epoch_loss = train_one_epoch(dataloader, model, optimizer, loss_fn)\n",
    "        avg_epoch_loss = epoch_loss / n_batches\n",
    "        print(f\"Epoch: {epoch}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        # Log metrics to wandb\n",
    "        wandb.log({\"epoch\": epoch, \"avg_epoch_loss\": avg_epoch_loss})\n",
    "    return avg_epoch_loss\n",
    "\n",
    "def full_pipeline(data, word_embeddings_filename: str, \n",
    "                  batch_size: int,\n",
    "                  ngram: int,\n",
    "                  hidden_units: int = 128,\n",
    "                  epochs: int = 1,\n",
    "                  lr: float = 0.001,\n",
    "                  test_pct: float = 0.1, device: str = \"cpu\") -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the full training pipeline from loading embeddings to model training.\n",
    "    \n",
    "    Params:\n",
    "        data: Raw data as a list of lists of tokens (here, integer indices).\n",
    "        word_embeddings_filename: Filename for the pre-trained embeddings.\n",
    "        batch_size: Batch size for training.\n",
    "        ngram: N-gram size.\n",
    "        hidden_units: Number of hidden units.\n",
    "        epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "        test_pct: Percentage of data for testing (not used in training).\n",
    "    \n",
    "    Returns:\n",
    "        The trained FFNN model.\n",
    "    \"\"\"\n",
    "    # Load embeddings and create an embedder.\n",
    "    token_embeddings = nutils.load_word2vec(word_embeddings_filename)\n",
    "    embedder = nutils.create_embedder(token_embeddings)\n",
    "    \n",
    "    # Preprocess data.\n",
    "    encoded_tokens = encode_tokens(data, embedder)\n",
    "    vocab_size = embedder.num_embeddings\n",
    "    training_sample = generate_ngram_training_samples(encoded_tokens, ngram)\n",
    "    x_sample, y_sample = split_sequences(training_sample)\n",
    "    dataloader_train, _ = create_dataloaders(x_sample, y_sample, batch_size, test_pct)\n",
    "    \n",
    "    # Initialize the model.\n",
    "    model = FFNN(vocab_size=vocab_size, ngram=ngram, embedding_layer=embedder, hidden_units=hidden_units, device=device)\n",
    "\n",
    "    # Train the model.\n",
    "    final_loss = train(dataloader=dataloader_train, model=model, epochs=epochs, lr=lr)\n",
    "    \n",
    "    return model, final_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction and generation functions\n",
    "# -------------------------------\n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\t# Encode tokens\n",
    "    encoded_tokens = [model.embedding_layer.token_to_index[token] for token in input_tokens]\n",
    "    \n",
    "\t# Trasform to tensor\n",
    "    encoded_tokens = torch.tensor([encoded_tokens]).to(model.device) # Dim [1, ngram-1]\n",
    "    \n",
    "    # Setting model to evaluation mode turns off Dropout and BatchNorm making the predictions deterministic\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    \n",
    "    with torch.no_grad(): # Speeds up inference and reduces memory usage by not having to calcualte gradients\n",
    "        logits = model(encoded_tokens) # Forward pass on the model\n",
    "        probability = nn.functional.softmax(logits, dim=1) # Normalize z scores to probability\n",
    "        predicted_idx = torch.multinomial(probability, num_samples=1).item()\n",
    "\n",
    "        #predicted_idx = probability.argmax(dim=1).item() # Retrieve int value\n",
    "\t\t\n",
    "\t# Transform index to natural-language token\n",
    "    predicted_token = model.embedding_layer.index_to_token[predicted_idx] \n",
    "    \n",
    "    return predicted_token\n",
    "\n",
    "from typing import List\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: List[str], max_tokens: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\" \n",
    "    n_tokens = 0 # Count tokens that have been generated\n",
    "    tokens = seed.copy() # Copy of initial seed\n",
    "    end_token = \"<\\s>\"\n",
    "    \n",
    "    while True:\n",
    "        for_prediction = seed[-(model.ngram-1):]\n",
    "        predicted_token = predict(model, for_prediction)\n",
    "        if predicted_token == end_token:\n",
    "        \tbreak\n",
    "        tokens.append(predicted_token)\n",
    "        n_tokens += 1\n",
    "        if max_tokens is not None and n_tokens >= max_tokens:\n",
    "            break\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def generate_sentences(model, seed: List[str],  n_sentences: int, max_tokens: int = None) -> List[str]:\n",
    "    return [generate(model, seed, max_tokens) for i in range(n_sentences)]\n",
    "\n",
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences\n",
    "\n",
    "def format_sentence(tokens_list: List[List[str]], by_char = False) -> str:\n",
    "  \"\"\"Removes <s> at the start of the sentence and </s> at ehe end. Joins the list of tokens into a string and capitalizes it.\n",
    "  Args:\n",
    "    tokens (list(list)): the list of tokens list to be formatted into a sentence\n",
    "\n",
    "  Returns:\n",
    "    string: formatted sentence as a string\n",
    "  \n",
    "  \"\"\"\n",
    "  text = \"\" # Initializing final sentence\n",
    "  for tokens in tokens_list: # Parsing through each individual sentence\n",
    "    while tokens[0] == '<s>': # Removes all <s> at the beggining even if there are several for ngram > 2 models\n",
    "      tokens.pop(0)\n",
    "    if tokens[-1] == '</s>': # Removes the one </s> at the end of the sentence\n",
    "      tokens.pop(-1)\n",
    "    if by_char:\n",
    "      sentence = \"\".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    else:\n",
    "      sentence = \" \".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    text += sentence + \".\\n\" # Adds a period and space separator between sentences\n",
    "  return text.strip(\" \") # Removes the last space in the last sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit constants as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "OUTPUT_WORDS = 'generated_wordbased.txt' # The file to save your generated sentences for word-based LM\n",
    "OUTPUT_CHARS = 'generated_charbased.txt' # The file to save your generated sentences for char-based LM\n",
    "\n",
    "# you can update these file names if you want to depending on how you are exploring \n",
    "# hyperparameters\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "MODEL_FILE_WORD = f'spooky_author_model_word_{NGRAM}.pt' # The file to save your trained word-based neural LM to\n",
    "MODEL_FILE_CHAR = f'spooky_author_model_char_{NGRAM}.pt' # The file to save your trained char-based neural LM to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_word = nutils.train_word2vec(answer2, EMBEDDINGS_SIZE)\n",
    "nutils.save_word2vec(trained_word, EMBEDDING_SAVE_FILE_WORD)\n",
    "\n",
    "trained_char = train_word2vec(answer1, EMBEDDINGS_SIZE)\n",
    "nutils.save_word2vec(trained_char, EMBEDDING_SAVE_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "text_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db575e48eb034300bcf9ae2afad56824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epochs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Average Loss: 2.0807\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m base_char_model \u001b[38;5;241m=\u001b[39m \u001b[43mfull_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchar_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMBEDDING_SAVE_FILE_CHAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SEQUENCES_PER_BATCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNGRAM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m base_word_model \u001b[38;5;241m=\u001b[39m full_pipeline(data\u001b[38;5;241m=\u001b[39mtext_data, word_embeddings_filename\u001b[38;5;241m=\u001b[39mEMBEDDING_SAVE_FILE_WORD, batch_size\u001b[38;5;241m=\u001b[39mNUM_SEQUENCES_PER_BATCH, ngram\u001b[38;5;241m=\u001b[39mNGRAM, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 246\u001b[0m, in \u001b[0;36mfull_pipeline\u001b[1;34m(data, word_embeddings_filename, batch_size, ngram, hidden_units, epochs, lr, test_pct, device)\u001b[0m\n\u001b[0;32m    243\u001b[0m model \u001b[38;5;241m=\u001b[39m FFNN(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, ngram\u001b[38;5;241m=\u001b[39mngram, embedding_layer\u001b[38;5;241m=\u001b[39membedder, hidden_units\u001b[38;5;241m=\u001b[39mhidden_units, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Train the model.\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloader_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, final_loss\n",
      "Cell \u001b[1;32mIn[5], line 205\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, epochs, lr)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_epoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# Log metrics to wandb\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_epoch_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_epoch_loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_epoch_loss\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\wandb\\sdk\\lib\\preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "base_char_model = full_pipeline(data=char_data, word_embeddings_filename=EMBEDDING_SAVE_FILE_CHAR, batch_size=NUM_SEQUENCES_PER_BATCH, ngram=NGRAM, epochs=1)\n",
    "base_word_model = full_pipeline(data=text_data, word_embeddings_filename=EMBEDDING_SAVE_FILE_WORD, batch_size=NUM_SEQUENCES_PER_BATCH, ngram=NGRAM, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t# Initialize a wandb run (hyperparameters come from wandb.config).\n",
    "\twandb.init(\n",
    "\t\tentity = \"northeastern-university\",\n",
    "\t\tproject = \"neural-language-model\"\n",
    "\t\t)\n",
    "\t\n",
    "\tconfig = wandb.config\n",
    "\n",
    "\tEMBEDDINGS_SIZE = config.embeddings_size\n",
    "\tNGRAM = config.ngram\n",
    "\tNUM_SEQUENCES_PER_BATCH = config.batch_size\n",
    "\tHIDDEN_UNITS = config.hidden_units\n",
    "\tEPOCHS = config.epochs\n",
    "\tLR = config.lr\n",
    "\n",
    "\tTRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "\n",
    "\tEMBEDDING_SAVE_FILE_WORD = f\"embeddings/spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "\tEMBEDDING_SAVE_FILE_CHAR = f\"embeddings/spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "\tMODEL_FILE_WORD = f'models/spooky_author_model_word_{EMBEDDINGS_SIZE}_{NUM_SEQUENCES_PER_BATCH}_{NGRAM}_{HIDDEN_UNITS}_{EPOCHS}_{LR}.pt' # The file to save your trained word-based neural LM to\n",
    "\tMODEL_FILE_CHAR = f'models/spooky_author_model_char_{EMBEDDINGS_SIZE}_{NUM_SEQUENCES_PER_BATCH}_{NGRAM}_{HIDDEN_UNITS}_{EPOCHS}_{LR}.pt' # The file to save your trained char-based neural LM to\n",
    "\n",
    "\tif config.text_type == \"word\":\n",
    "\t\tdata = nutils.load_word2vec(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\t\tword_embeddings_filename = EMBEDDING_SAVE_FILE_WORD\n",
    "\t\tif not os.path.exists(word_embeddings_filename):\n",
    "\t\t\ttrained_word = nutils.train_word2vec(data, EMBEDDINGS_SIZE)\n",
    "\t\t\tnutils.save_word2vec(trained_word, EMBEDDING_SAVE_FILE_WORD)\n",
    "\n",
    "\telif config.text_type == \"char\":\n",
    "\t\tdata = nutils.load_word2vec(TRAIN_FILE, NGRAM, by_character=True)\n",
    "\t\tword_embeddings_filename = EMBEDDING_SAVE_FILE_CHAR\n",
    "\t\tif not os.path.exists(word_embeddings_filename):\n",
    "\t\t\ttrained_char = nutils.train_word2vec(data, EMBEDDINGS_SIZE)\n",
    "\t\t\tnutils.save_word2vec(trained_char, EMBEDDING_SAVE_FILE_CHAR)\n",
    "\n",
    "\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\tmodel, final_loss = full_pipeline(\n",
    "\t\tdata=data,\n",
    "\t\tword_embeddings_filename = word_embeddings_filename,\n",
    "\t\tbatch_size=NUM_SEQUENCES_PER_BATCH,\n",
    "\t\tngram=NGRAM,\n",
    "\t\thidden_units=config.hidden_units,\n",
    "\t\tepochs=config.epochs,\n",
    "\t\tlr=config.lr,\n",
    "\t\ttest_pct=config.test_pct,\n",
    "\t\tdevice = device\n",
    "\t)\n",
    "\n",
    "\tif config.text_type == \"word\":\n",
    "\t\ttorch.save(model.state_dict(), MODEL_FILE_WORD)\n",
    "\n",
    "\telif config.text_type == \"char\":\n",
    "\t\ttorch.save(model.state_dict(), MODEL_FILE_CHAR)\n",
    "\n",
    "\twandb.log({\"final_loss\": final_loss})\n",
    "\twandb.finish()\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sweep configuration\n",
    "sweep_config = {\n",
    "    \"name\": \"word_hyperparameter_sweep\",\n",
    "\t\"method\": \"grid\",  # Options: \"grid\", \"random\", \"bayes\"\n",
    "\t\"metric\": {\n",
    "\t\t\"name\": \"avg_epoch_loss\",\n",
    "\t\t\"goal\": \"minimize\"  # We want to minimize the training loss.\n",
    "\t},\n",
    "\t\"parameters\": {\n",
    "\t\t\"embeddings_size\": {\"values\": [50, 100, 150, 200]},\n",
    "\t\t\"batch_size\": {\"values\": [16, 32, 64]},\n",
    "\t\t\"ngram\": {\"values\": [2, 3, 4, 5]},\n",
    "\t\t\"hidden_units\": {\"values\": [64, 128, 256]},\n",
    "\t\t\"epochs\": {\"values\": [5, 10, 25]},\n",
    "\t\t\"lr\": {\"values\": [0.001, 0.0001]},\n",
    "\t\t\"test_pct\": {\"value\": 0.1},  # Fixed value.\n",
    "\t\t\"text_type\": {\"value\": [\"word\"]}\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: q44i6ugk\n",
      "Sweep URL: https://wandb.ai/biofx/neural-language-model/sweeps/q44i6ugk\n",
      "Sweep ID: q44i6ugk\n"
     ]
    }
   ],
   "source": [
    "# Register the sweep with wandb.\n",
    "sweep_id = wandb.sweep(sweep_config,\n",
    "                       project=\"neural-language-model\")\n",
    "print(\"Sweep ID:\", sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set count to the number of runs you wish to execute; here, 5 runs are used as an example.\n",
    "wandb.agent(sweep_id, function=main, count=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
