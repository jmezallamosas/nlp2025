{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8dcfc9-0378-4629-8f9f-58e64f121c3b",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634622b-b20d-43a3-a3f0-d6adc6ab64fe",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: __YOUR NAMES HERE__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47258c1-5462-4f83-afc5-2badaeb4c33d",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (80 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f24a57-464a-4194-9fee-36411939e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# This function gives us nice print-outs of our models.\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff97dc8-e587-46c6-8eff-69b9c41b6099",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit constants as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "OUTPUT_WORDS = 'generated_wordbased.txt' # The file to save your generated sentences for word-based LM\n",
    "OUTPUT_CHARS = 'generated_charbased.txt' # The file to save your generated sentences for char-based LM\n",
    "\n",
    "# you can update these file names if you want to depending on how you are exploring \n",
    "# hyperparameters\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "MODEL_FILE_WORD = f'spooky_author_model_word_{NGRAM}.pt' # The file to save your trained word-based neural LM to\n",
    "MODEL_FILE_CHAR = f'spooky_author_model_char_{NGRAM}.pt' # The file to save your trained char-based neural LM to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6669b3-3df5-4d80-b67c-7c8c5f5fd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your word vectors that you made in your previous notebook AND \n",
    "# use the create_embedder function to make your pytorch embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b664f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll also need to re-load your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578103c1-6388-4f3b-abbc-54e459c1ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to vectorize a text corpus. \n",
    "# Here, it creates a mapping from word to that word's unique index.\n",
    "\n",
    "# Hint: use one of the dicts from your embedding function.\n",
    "\n",
    "def encode_tokens(data: list[list[str]], embedder: torch.nn.Embedding) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode your data from tokens to integers for both word and char embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296f18f-8f8f-41a7-85dc-b3661f12feba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the size of the mappings for each of your embedders.\n",
    "# these should match the vocab sizes you calculated in Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbd01e-d778-4542-8bb9-9086cdf4c06e",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556a723-7cfd-4f86-a424-0ba2e15acfb5",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences\n",
    "\n",
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (y).\n",
    "\n",
    "Example: this process however afforded me\n",
    "\n",
    "Would become:\n",
    "```\n",
    "X\n",
    "[[this,    process]\n",
    "[process, however]\n",
    "[however, afforded]]\n",
    "\n",
    "y\n",
    "[however,\n",
    "afforded,\n",
    "me]\n",
    "```\n",
    "\n",
    "\n",
    "Our first step is to generate n-grams like we have always been doing. We'll just do this \n",
    "on our encoded data instead of the raw text. (Feel free to consult your past HW here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0fc28-e667-4d72-9be2-afd749cb9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "    pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d80353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [0, 0, 31]\n",
    "# [0, 31, 2959]\n",
    "# [31, 2959, 2]\n",
    "# ...\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [20, 20, 2]\n",
    "# [20, 2, 8]\n",
    "# [2, 8, 6]\n",
    "# ...\n",
    "\n",
    "# print out the first 5 training samples for each and make sure that the \n",
    "# windows are sliding one word at a time. These should be integers!\n",
    "# make sure that they map to the correct words in your vocab\n",
    "# Hint: what word maps to token 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb275f6c-bff7-465f-b4c3-759610d44113",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a DataLoader (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ecf47-13ad-403d-a5e2-e19d462aab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note here that each sequence we've created so far is in the form:\n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate them into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here for both word and character data\n",
    "# you can write a function to do this if you'd like (not required, might be helpful)\n",
    "\n",
    "\n",
    "# print out the shapes (or lengths to know how many sequences there are and how many\n",
    "# elements each sub-list has) for word-based to verify that they are correct\n",
    "\n",
    "# print out the shapes for char-based to verify that they are correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099f7f6-be95-49f2-89e6-fa7833fcb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ef8b",
   "metadata": {},
   "source": [
    "### some definitions:\n",
    "- a single __batch__ is the number of sequences that your model will evaluate at once when it learns\n",
    "-  __steps per epoch__ is the number of batches that your model will see in a single epoch  (one pass through the data)-- your NUM_SEQUENCES_PER_BATCH constant is the batch size--you won't need this for pytorch but it's useful to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f96706-8a23-4ca7-82ab-35d9f47a1a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your dataloaders for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is \n",
    "# correct for both word and character data\n",
    "# note that your train data and your test data should have the same shapes!\n",
    "# print enough information to verify that the shapes are correct\n",
    "\n",
    "\n",
    "# Examples:\n",
    "# Normally you would loop over your dataloader, but we just want to get a single batch to test it out:\n",
    "# Every time you call next, you advance to the next batch\n",
    "# sample_X, sample_y = next(iter(train_dataloader))\n",
    "# sample_X.shape # (batch_size, (n-1)*EMBEDDING_SIZE)  \n",
    "# sample_y.shape  # (batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207dc63-8e68-47f1-b3f7-daf0c0d855f9",
   "metadata": {},
   "source": [
    "### d) Define, train & save your models (25 points)\n",
    "\n",
    "Write the code to train feedforward neural language models for both word embeddings and character embeddings make sure not to just copy + paste to train your two models (define functions as needed).\n",
    "\n",
    "Define your model architecture using PyTorch layers and activation functions. When training, use the Adam optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) instead of sgd (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD).\n",
    "\n",
    "add cells as desired :)\n",
    "\n",
    "Your FFNN should have the following architecture:\n",
    "- It should be a two layer neural net (one hidden layer, one output layer)\n",
    "- It should use ReLU as its activation function\n",
    "\n",
    "Our biggest piece of advice--make sure that you understand what dimensions each layer needs to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac10d29e-5af0-46b8-a2e6-a96832b7af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing our implementation of a Feed-Forward Neural Network.\n",
    "    You will need to implement two methods:\n",
    "        - A constructor to set up the architecture and hyperparameters of the model\n",
    "        - The forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model. \n",
    "        \n",
    "        You can change these parameters as you would like.\n",
    "        Once you get a working model, you are encouraged to\n",
    "        experiment with this constructor to improve performance.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: The number of words in the vocabulary\n",
    "            ngram: The value of N for training and prediction.\n",
    "            embedding_layer: The previously trained embedder. \n",
    "            hidden_units: The size of the hidden layer.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # we recommend saving the parameters as instance variables\n",
    "        # so you can access them later as needed\n",
    "        # (in addition to anything else you need to do here)\n",
    "    \n",
    "    def forward(self, X: list) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the network.\n",
    "        This is not a prediction, and it should not apply softmax.\n",
    "\n",
    "        Params:\n",
    "            X: the input data\n",
    "\n",
    "        Returns:\n",
    "            The output of the model; i.e. its predictions.\n",
    "        \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b6a91-2c8b-4072-815d-fd83b0a6fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Our model's training loop.\n",
    "    Print the cross entropy loss every epoch.\n",
    "    You should use the Adam optimizer instead of SGD.\n",
    "\n",
    "    When looking for documentation, try to stay on PyTorch's website.\n",
    "    This might be a good place to start: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html \n",
    "    They should have plenty of tutorials, and we don't want you to get confused from other resources.\n",
    "\n",
    "    Params:\n",
    "        dataloader: The training dataloader\n",
    "        model: The model we wish to train\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: Learning rate \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # you will need to initialize an optimizer and a loss function, which you should do\n",
    "    # before the training loop\n",
    "\n",
    "    # print out the epoch number and the current average loss after each epoch\n",
    "    # you can use tqdm to print out a progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b981e7",
   "metadata": {},
   "source": [
    "For the next part, we're testing our model's functions so we can see if it works.\n",
    "No need to do this on both the word and character data, just one is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your model\n",
    "# Print out its architecture (use the imported summary function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2d1d4-c860-46b6-a44e-c1627a2d77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 points\n",
    "\n",
    "# train your models for 1 epoch\n",
    "# see timing information posted on Canvas!\n",
    "\n",
    "# re-create your data loader fresh\n",
    "\n",
    "# train your model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755443f",
   "metadata": {},
   "source": [
    "10. You're reporting the loss after each epoch of training. What is the loss for your model after 1 epoch?\n",
    "- word or character-based? __YOUR ANSWER HERE__\n",
    "- loss? __YOUR ANSWER HERE__\n",
    "\n",
    "Loss isn't accuracy, but it does tell us whether or not the model is improving over time. For character-based, loss after one epoch should be ~2.1; for word-based it is ~5.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31e2d",
   "metadata": {},
   "source": [
    "### e) create a full pipeline (13 points)\n",
    "\n",
    "We've made all the pieces that you'll need for a full pipeline, now let's package everything together nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22dfd78-fc75-41eb-ba67-17b1141ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "# make a function that does your full *training* pipeline\n",
    "# This is essentially pulling the pieces that you've done so far earlier in this \n",
    "# notebook into a single function that you can call to train your model\n",
    "\n",
    "\n",
    "def full_pipeline(data: list[list[str]], word_embeddings_filename: str, \n",
    "                batch_size:int = NUM_SEQUENCES_PER_BATCH,\n",
    "                ngram:int = NGRAM, hidden_units = 128, epochs = 1,\n",
    "                lr = 0.001, test_pct = 0.1,\n",
    "                ) -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from loading embeddings to training.\n",
    "    You won't use the test set for anything.\n",
    "\n",
    "    Params:\n",
    "        data: The raw data to train on, parsed as a list of lists of tokens\n",
    "        word_embeddings_filename: The filename of the Word2Vec word embeddings\n",
    "        batch_size: The batch size to use\n",
    "        hidden_units: The number of hidden units to use\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: The learning rate to use\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24546581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Use your full pipeline to train models on the word data and the character data.\n",
    "# Feel free to add cells if you'd like to.\n",
    "\n",
    "# Train your models however you'd like. Play around with number of epochs, learning rate, etc.\n",
    "# Do whatever you'd like to for exploring hyperparameters.\n",
    "# You aren't required to hit a certain loss, but you should leave code here that shows\n",
    "# that you explored effects of changing at least two of the different hyperparameters\n",
    "# Please don't change the architecture of the model (keep it a 2-layer model with 1 hidden layer)\n",
    "\n",
    "# You'll likely want to do this exploration AFTER completing your prediction and generation code, so start\n",
    "# with just training for 1 - 5 epochs with default params.\n",
    "\n",
    "\n",
    "# Word-based takes Felix's computer 7 - 8 min for 5 epochs with default params running on CPU\n",
    "# Char-based Felix's computer ~1min 30sec - 2min for 5 epochs with default params running on CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1171c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you're happy with them, save both models\n",
    "# Feel free to play around with any hyperparameters you'd like\n",
    "\n",
    "# using torch.save and the model's state_dict\n",
    "# torch.save(word_model.state_dict(), MODEL_FILE_WORD)\n",
    "# torch.save(char_model.state_dict(), MODEL_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e554fe9-a413-4471-96fe-a51e3764c2ae",
   "metadata": {},
   "source": [
    "### f) Generate Sentences (25 points)\n",
    "\n",
    "Now that you have trained models, you'll work on the generation piece. Note that because you saved your models, even if you have to re-start your kernel, you should be able to re-load them without having to re-train them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models in again with code like:\n",
    "# model = FFNN(same params as when you created the model to begin with)\n",
    "# model.load_state_dict(torch.load(MODEL_FILE))\n",
    "# then switch the model into evaluation mode\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608b05fb-8db9-4d44-8cb2-bb9ec5f373d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points \n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721708d-3dde-4e91-888d-080c4ac6ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: list[str], max_tokens: int = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b317a-e8c7-41d4-a598-34318ffda93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# generate and display ten sequences from both your word model and your character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# For character-based, replace _ with a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a1ec4-9a8b-46dc-894d-b41be323058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Generate 100 example sentences with each model and save them to two files, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "# We've defined the filenames for you at the top of this notebook\n",
    "# Do not print these sentences here :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef7860",
   "metadata": {},
   "source": [
    "11. What were the final parameters that you used for your model? \n",
    "- N: __YOUR ANSWER HERE__\n",
    "- embedding size: __YOUR ANSWER HERE__\n",
    "- epochs: __YOUR ANSWER HERE__\n",
    "- hidden units: __YOUR ANSWER HERE__\n",
    "- learning rate: __YOUR ANSWER HERE__\n",
    "- training time + system you were running it on (operating system + chip/specs): __YOUR ANSWER HERE__\n",
    "    - for pairs, you can either note both partners' training times or just one\n",
    "\n",
    "- What was the word-based model's final loss? __YOUR ANSWER HERE__\n",
    "- Character based? __YOUR ANSWER HERE__\n",
    "\n",
    "If you used different parameters for your word-based and character-based models, note the different parameters clearly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
