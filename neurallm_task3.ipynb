{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8dcfc9-0378-4629-8f9f-58e64f121c3b",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0634622b-b20d-43a3-a3f0-d6adc6ab64fe",
   "metadata": {},
   "source": [
    "### Names\n",
    "----\n",
    "Names: __Katherine Aristizabal, Jose Meza Llamosas__ (Write these in every notebook you submit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47258c1-5462-4f83-afc5-2badaeb4c33d",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (80 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84f24a57-464a-4194-9fee-36411939e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "\n",
    "import numpy as np\n",
    "from typing import List\n",
    "# if you want fancy progress bars\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# This function gives us nice print-outs of our models.\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff97dc8-e587-46c6-8eff-69b9c41b6099",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fcca314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit constants as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3\n",
    "NUM_SEQUENCES_PER_BATCH = 128\n",
    "\n",
    "TRAIN_FILE = 'spooky_author_train.csv' # The file to train your language model on\n",
    "OUTPUT_WORDS = 'generated_wordbased.txt' # The file to save your generated sentences for word-based LM\n",
    "OUTPUT_CHARS = 'generated_charbased.txt' # The file to save your generated sentences for char-based LM\n",
    "\n",
    "# you can update these file names if you want to depending on how you are exploring \n",
    "# hyperparameters\n",
    "EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_{EMBEDDINGS_SIZE}.model\" # The file to save your word embeddings to\n",
    "EMBEDDING_SAVE_FILE_CHAR = f\"spooky_embedding_char_{EMBEDDINGS_SIZE}.model\" # The file to save your char embeddings to\n",
    "MODEL_FILE_WORD = f'spooky_author_model_word_{NGRAM}.pt' # The file to save your trained word-based neural LM to\n",
    "MODEL_FILE_CHAR = f'spooky_author_model_char_{NGRAM}.pt' # The file to save your trained char-based neural LM to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6669b3-3df5-4d80-b67c-7c8c5f5fd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your word vectors that you made in your previous notebook AND \n",
    "# use the create_embedder function to make your pytorch embedder\n",
    "char_wv = nutils.load_word2vec(EMBEDDING_SAVE_FILE_CHAR)\n",
    "char_embedder = nutils.create_embedder(char_wv)\n",
    "\n",
    "word_wv = nutils.load_word2vec(EMBEDDING_SAVE_FILE_WORD)\n",
    "word_embedder = nutils.create_embedder(word_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d452629",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'spooky_embedding_word_100.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m FINAL_EMBEDDING_SAVE_FILE_WORD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspooky_embedding_word_100.model\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# The file to save your word embeddings to\u001b[39;00m\n\u001b[0;32m      3\u001b[0m char_embedder \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mcreate_embedder(char_wv)\n\u001b[1;32m----> 5\u001b[0m fword_wv \u001b[38;5;241m=\u001b[39m \u001b[43mnutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFINAL_EMBEDDING_SAVE_FILE_WORD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m fword_embedder \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mcreate_embedder(word_wv)\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\Documents\\Desktop\\Northeastern_University\\Academic_Classes\\2025_1\\NLP\\nlp2025\\neurallm_utils.py:121\u001b[0m, in \u001b[0;36mload_word2vec\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_word2vec\u001b[39m(filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Word2Vec:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Loads weights of trained gensim Word2Vec model from a file.\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    Params:\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m        filename: The saved model file.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWord2Vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\gensim\\models\\word2vec.py:1953\u001b[0m, in \u001b[0;36mWord2Vec.load\u001b[1;34m(cls, rethrow, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a previously saved :class:`~gensim.models.word2vec.Word2Vec` model.\u001b[39;00m\n\u001b[0;32m   1935\u001b[0m \n\u001b[0;32m   1936\u001b[0m \u001b[38;5;124;03mSee Also\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1950\u001b[0m \n\u001b[0;32m   1951\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1953\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(Word2Vec, \u001b[38;5;28mcls\u001b[39m)\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, Word2Vec):\n\u001b[0;32m   1955\u001b[0m         rethrow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\gensim\\utils.py:485\u001b[0m, in \u001b[0;36mSaveLoad.load\u001b[1;34m(cls, fname, mmap)\u001b[0m\n\u001b[0;32m    481\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, fname)\n\u001b[0;32m    483\u001b[0m compress, subname \u001b[38;5;241m=\u001b[39m SaveLoad\u001b[38;5;241m.\u001b[39m_adapt_by_suffix(fname)\n\u001b[1;32m--> 485\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    486\u001b[0m obj\u001b[38;5;241m.\u001b[39m_load_specials(fname, mmap, compress, subname)\n\u001b[0;32m    487\u001b[0m obj\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded\u001b[39m\u001b[38;5;124m\"\u001b[39m, fname\u001b[38;5;241m=\u001b[39mfname)\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\gensim\\utils.py:1459\u001b[0m, in \u001b[0;36munpickle\u001b[1;34m(fname)\u001b[0m\n\u001b[0;32m   1445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munpickle\u001b[39m(fname):\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load object from `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1457\u001b[0m \n\u001b[0;32m   1458\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1459\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1460\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _pickle\u001b[38;5;241m.\u001b[39mload(f, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\mezallamosas.j\\AppData\\Local\\miniconda3\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:375\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    373\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spooky_embedding_word_100.model'"
     ]
    }
   ],
   "source": [
    "FINAL_EMBEDDING_SAVE_FILE_WORD = f\"spooky_embedding_word_100.model\" # The file to save your word embeddings to\n",
    "\n",
    "char_embedder = nutils.create_embedder(char_wv)\n",
    "\n",
    "fword_wv = nutils.load_word2vec(FINAL_EMBEDDING_SAVE_FILE_WORD)\n",
    "fword_embedder = nutils.create_embedder(word_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b664f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll also need to re-load your text data\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "text_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "578103c1-6388-4f3b-abbc-54e459c1ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to vectorize a text corpus. \n",
    "# Here, it creates a mapping from word to that word's unique index.\n",
    "\n",
    "# Hint: use one of the dicts from your embedding function.\n",
    "\n",
    "def encode_tokens(data: List[List[str]], embedder: torch.nn.Embedding) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Replaces each natural-language token with its embedder index.\n",
    "\n",
    "    e.g. [[\"<s>\", \"once\", \"upon\", \"a\", \"time\"],\n",
    "          [\"there\", \"was\", \"a\", ]]\n",
    "        ->\n",
    "        [[0, 59, 203, 1, 126],\n",
    "         [26, 15, 1]]\n",
    "        (The indices are arbitrary, as they are dependent on your embedder)\n",
    "\n",
    "    Params:\n",
    "        data: The corpus\n",
    "        embedder: An embedder trained on the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    finalList = []\n",
    "    for list in data:\n",
    "        currList = []\n",
    "        for word in list:\n",
    "            index = embedder.token_to_index[word]\n",
    "            currList.append(index)\n",
    "        finalList.append(currList)\n",
    "\n",
    "    return finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "561ad693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode your data from tokens to integers for both word and char embeddings\n",
    "encoded_chars = encode_tokens(char_data, char_embedder)\n",
    "encoded_word = encode_tokens(text_data, word_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2296f18f-8f8f-41a7-85dc-b3661f12feba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char embedder size 60\n",
      "word embedder size 25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the mappings for each of your embedders.\n",
    "# these should match the vocab sizes you calculated in Task 2\n",
    "\n",
    "# 4 points\n",
    "# print out the vocabulary size for your embeddings for both your word\n",
    "# embeddings and your character embeddings\n",
    "# label which is which when you print them out\n",
    "\n",
    "char_vocab_size = len(char_embedder.token_to_index)\n",
    "word_vocab_size = len(word_embedder.token_to_index)\n",
    "\n",
    "\n",
    "print(f\"char embedder size {char_vocab_size}\")\n",
    "print(f\"word embedder size {word_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffbd01e-d778-4542-8bb9-9086cdf4c06e",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7556a723-7cfd-4f86-a424-0ba2e15acfb5",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences\n",
    "\n",
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (y).\n",
    "\n",
    "Example: this process however afforded me\n",
    "\n",
    "Would become:\n",
    "```\n",
    "X\n",
    "[[this,    process]\n",
    "[process, however]\n",
    "[however, afforded]]\n",
    "\n",
    "y\n",
    "[however,\n",
    "afforded,\n",
    "me]\n",
    "```\n",
    "\n",
    "\n",
    "Our first step is to generate n-grams like we have always been doing. We'll just do this \n",
    "on our encoded data instead of the raw text. (Feel free to consult your past HW here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9e0fc28-e667-4d72-9be2-afd749cb9832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_ngrams(tokens: list, n: int) -> list:\n",
    "    \"\"\"Creates n-grams for the given token sequence.\n",
    "    Args:\n",
    "      tokens (list): a list of tokens as strings\n",
    "      n (int): the length of n-grams to create\n",
    "\n",
    "    Returns:\n",
    "      list: list of tuples of strings, each tuple being one of the individual n-grams\n",
    "    \"\"\"\n",
    "    # STUDENTS IMPLEMENT\n",
    "    res = []\n",
    "    for i in range(0, len(tokens)-n):\n",
    "        #append n gram + yth value\n",
    "        res.append(tokens[i:i+n+1])\n",
    "    return res\n",
    "\n",
    "def generate_ngram_training_samples(encoded: list[list[int]], ngram: int) -> list:\n",
    "    \"\"\"\n",
    "    Takes the **encoded** data (list of lists of ints) and \n",
    "    generates the training samples out of it.\n",
    "    \n",
    "    Parameters:\n",
    "        up to you, we've put in what we used\n",
    "        but you can add/remove as needed\n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    \"\"\"\n",
    "\n",
    "    #1 2 3 4\n",
    "    #[1,2, y=3]\n",
    "    #[2,3, y=4]\n",
    "\n",
    "    # if you'd like to use tqdm, you can use it like this:\n",
    "    # for i in tqdm(range(len(encoded))):\n",
    "    final_list = []\n",
    "    for list in encoded:\n",
    "        currList = create_ngrams(list, ngram-1)\n",
    "        final_list.extend(currList)\n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6d80353",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoded_chars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# generate your training samples for both word and character data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# print out the first 5 training samples for each\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# we have displayed the number of sequences\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# to expect for both characters and words\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m char_sample \u001b[38;5;241m=\u001b[39m generate_ngram_training_samples(\u001b[43mencoded_chars\u001b[49m, NGRAM)\n\u001b[0;32m      7\u001b[0m word_sample \u001b[38;5;241m=\u001b[39m generate_ngram_training_samples(encoded_word, NGRAM)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength char  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(char_sample)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded_chars' is not defined"
     ]
    }
   ],
   "source": [
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "char_sample = generate_ngram_training_samples(encoded_chars, NGRAM)\n",
    "word_sample = generate_ngram_training_samples(encoded_word, NGRAM)\n",
    "print(f\"length char  {len(char_sample)}\")\n",
    "print(f\"length word  {len(word_sample)}\")\n",
    "\n",
    "print(char_sample[0:5])\n",
    "print(word_sample[0:5])\n",
    "\n",
    "finalList = []\n",
    "for list in char_sample:\n",
    "    currList = []\n",
    "    for i in list:\n",
    "        tok = char_embedder.index_to_token[i]\n",
    "        currList.append(tok)\n",
    "    finalList.append(currList)\n",
    "print( finalList[0:5])\n",
    "\n",
    "finalList = []\n",
    "for list in char_sample:\n",
    "    currList = []\n",
    "    for i in list:\n",
    "        tok = word_embedder.index_to_token[i]\n",
    "        currList.append(tok)\n",
    "    finalList.append(currList)\n",
    "print( finalList[0:5])\n",
    "\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [0, 0, 31]\n",
    "# [0, 31, 2959]\n",
    "# [31, 2959, 2]\n",
    "# ...\n",
    "\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [20, 20, 2]\n",
    "# [20, 2, 8]\n",
    "# [2, 8, 6]\n",
    "# ...\n",
    "\n",
    "# print out the first 5 training samples for each and make sure that the \n",
    "# windows are sliding one word at a time. These should be integers!\n",
    "# make sure that they map to the correct words in your vocab\n",
    "# Hint: what word maps to token 0?\n",
    "print(word_embedder.token_to_index[','])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb275f6c-bff7-465f-b4c3-759610d44113",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a DataLoader (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "531ecf47-13ad-403d-a5e2-e19d462aab97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'char_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m x_char \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m y_char \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mchar_sample\u001b[49m:\n\u001b[0;32m     18\u001b[0m     x_char\u001b[38;5;241m.\u001b[39mappend(line[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     19\u001b[0m     y_char\u001b[38;5;241m.\u001b[39mappend(line[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'char_sample' is not defined"
     ]
    }
   ],
   "source": [
    "# Note here that each sequence we've created so far is in the form:\n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate them into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here for both word and character data\n",
    "# you can write a function to do this if you'd like (not required, might be helpful)\n",
    "\n",
    "def split_sequences(training_sample):\n",
    "    x_sample = []\n",
    "    y_sample = []\n",
    "    for line in training_sample:\n",
    "        x_sample.append(line[0:-1])\n",
    "        y_sample.append(line[-1])\n",
    "    return x_sample, y_sample\n",
    "\n",
    "x_char = []\n",
    "y_char = []\n",
    "for line in char_sample:\n",
    "    x_char.append(line[0:-1])\n",
    "    y_char.append(line[-1])\n",
    "\n",
    "x_word = []\n",
    "y_word = []\n",
    "for line in word_sample:\n",
    "    x_word.append(line[0:-1])\n",
    "    y_word.append(line[-1])\n",
    "\n",
    "\"\"\"print(char_sample[0:5])\n",
    "def split_training_data(training: list[list[int]]) -> list:\n",
    "\n",
    "    Takes the training data and splits it into X and y.\n",
    "    tuple: a tuple of two lists, one for X and one for y\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for l in training:\n",
    "        print(l)\n",
    "        x.append(l[0:-1])\n",
    "        y.append(l[-1])\n",
    "    return x, y\"\"\"\n",
    "# print out the shapes (or lengths to know how many sequences there are and how many\n",
    "# elements each sub-list has) for word-based to verify that they are correct\n",
    "\n",
    "print(f\"word x length {len(x_word)}\")\n",
    "length_list =[]\n",
    "for line in x_word:\n",
    "  #  print(line)\n",
    "    length_list.append(len(line))\n",
    "\n",
    "print(length_list)\n",
    "\n",
    "print(f\"word y length {len(y_word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b099f7f6-be95-49f2-89e6-fa7833fcb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X: list, y: list, num_sequences_per_batch: int, \n",
    "                       test_pct: float = 0.1, shuffle: bool = True) -> tuple[torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Convert our data into a PyTorch DataLoader.    \n",
    "    A DataLoader is an object that splits the dataset into batches for training.\n",
    "    PyTorch docs: \n",
    "        https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "        https://pytorch.org/docs/stable/data.html\n",
    "\n",
    "    Note that you have to first convert your data into a PyTorch DataSet.\n",
    "    You DO NOT have to implement this yourself, instead you should use a TensorDataset.\n",
    "\n",
    "    You are in charge of splitting the data into train and test sets based on the given\n",
    "    test_pct. There are several functions you can use to acheive this!\n",
    "\n",
    "    The shuffle parameter refers to shuffling the data *in the loader* (look at the docs),\n",
    "    not whether or not to shuffle the data before splitting it into train and test sets.\n",
    "    (don't shuffle before splitting)\n",
    "\n",
    "    Params:\n",
    "        X: A list of input sequences\n",
    "        Y: A list of labels\n",
    "        num_sequences_per_batch: Batch size\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "        shuffle: INSTRUCTORS ONLY\n",
    "\n",
    "    Returns:\n",
    "        One DataLoader for training, and one for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataSet = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    test_size = int(len(dataSet)*test_pct)\n",
    "    train_size = len(dataSet) - test_size\n",
    "    train_data, test_data = torch.utils.data.random_split(dataSet, [train_size, test_size])\n",
    "    dataloader_train = DataLoader(train_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    dataloader_test = DataLoader(test_data, batch_size=num_sequences_per_batch, shuffle=shuffle)\n",
    "    return dataloader_train, dataloader_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7ef8b",
   "metadata": {},
   "source": [
    "### some definitions:\n",
    "- a single __batch__ is the number of sequences that your model will evaluate at once when it learns\n",
    "-  __steps per epoch__ is the number of batches that your model will see in a single epoch  (one pass through the data)-- your NUM_SEQUENCES_PER_BATCH constant is the batch size--you won't need this for pytorch but it's useful to know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f96706-8a23-4ca7-82ab-35d9f47a1a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word data loader train shape for X: torch.Size([128, 2]) and y: torch.Size([128])\n",
      "word data loader test shape for X: torch.Size([128, 2]) and y: torch.Size([128])\n",
      "\n",
      "char data loader train shape for X: torch.Size([128, 2]) and y: torch.Size([128])\n",
      "char data loader test shape for X: torch.Size([128, 2]) and y: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# initialize your dataloaders for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is \n",
    "# correct for both word and character data\n",
    "# note that your train data and your test data should have the same shapes!\n",
    "# print enough information to verify that the shapes are correct\n",
    "\n",
    "word_dataloader_train, word_dataloader_test = create_dataloaders(x_word, y_word, NUM_SEQUENCES_PER_BATCH)\n",
    "char_dataloader_train, char_dataloader_test = create_dataloaders(x_char, y_char, NUM_SEQUENCES_PER_BATCH)\n",
    "\n",
    "# Accesing the first batch of each data loader to print shape with next(iter())\n",
    "\n",
    "sample_x_word_train, sample_y_word_train = next(iter(word_dataloader_train))\n",
    "sample_x_word_test, sample_y_word_test = next(iter(word_dataloader_test))\n",
    "\n",
    "sample_x_char_train, sample_y_char_train = next(iter(char_dataloader_train))\n",
    "sample_x_char_test, sample_y_char_test = next(iter(char_dataloader_test))\n",
    "\n",
    "print(f\"word data loader train shape for X: {sample_x_word_train.shape} and y: {sample_y_word_train.shape}\")\n",
    "print(f\"word data loader test shape for X: {sample_x_word_test.shape} and y: {sample_y_word_test.shape}\\n\")\n",
    "\n",
    "print(f\"char data loader train shape for X: {sample_x_char_train.shape} and y: {sample_y_char_train.shape}\")\n",
    "print(f\"char data loader test shape for X: {sample_x_char_test.shape} and y: {sample_y_char_test.shape}\")\n",
    "\n",
    "# Examples:\n",
    "# Normally you would loop over your dataloader, but we just want to get a single batch to test it out:\n",
    "# Every time you call next, you advance to the next batch\n",
    "# sample_X, sample_y = next(iter(train_dataloader))\n",
    "# sample_X.shape # (batch_size, (n-1)*EMBEDDING_SIZE) # Correction from Piazza it should be (batch_size, n-1)  \n",
    "# sample_y.shape  # (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207dc63-8e68-47f1-b3f7-daf0c0d855f9",
   "metadata": {},
   "source": [
    "### d) Define, train & save your models (25 points)\n",
    "\n",
    "Write the code to train feedforward neural language models for both word embeddings and character embeddings make sure not to just copy + paste to train your two models (define functions as needed).\n",
    "\n",
    "Define your model architecture using PyTorch layers and activation functions. When training, use the Adam optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) instead of sgd (https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD).\n",
    "\n",
    "add cells as desired :)\n",
    "\n",
    "Your FFNN should have the following architecture:\n",
    "- It should be a two layer neural net (one hidden layer, one output layer)\n",
    "- It should use ReLU as its activation function\n",
    "\n",
    "Our biggest piece of advice--make sure that you understand what dimensions each layer needs to be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac10d29e-5af0-46b8-a2e6-a96832b7af48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A class representing our implementation of a Feed-Forward Neural Network.\n",
    "    You will need to implement two methods:\n",
    "        - A constructor to set up the architecture and hyperparameters of the model\n",
    "        - The forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, ngram: int, embedding_layer: torch.nn.Embedding, hidden_units=128):\n",
    "        \"\"\"\n",
    "        Initialize a new untrained model. \n",
    "        \n",
    "        You can change these parameters as you would like.\n",
    "        Once you get a working model, you are encouraged to\n",
    "        experiment with this constructor to improve performance.\n",
    "        \n",
    "        Params:\n",
    "            vocab_size: The number of words in the vocabulary\n",
    "            ngram: The value of N for training and prediction.\n",
    "            embedding_layer: The previously trained embedder. \n",
    "            hidden_units: The size of the hidden layer.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # we recommend saving the parameters as instance variables\n",
    "        # so you can access them later as needed\n",
    "        # (in addition to anything else you need to do here)\n",
    "        \n",
    "\t\t# Saving parameters as instance variables\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ngram = ngram\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "\t\t# Save embedding size\n",
    "        embedding_size = embedding_layer.embedding_dim\n",
    "        \n",
    "\t\t# Defining layers\n",
    "        self.flatten = nn.Flatten() # Useful later to flatten array of ngram-1 after embedding before passing it to the linear layer\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "\t\t\tnn.Linear(in_features=(ngram-1)*embedding_size, out_features=hidden_units, bias=True),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(in_features=hidden_units, out_features=vocab_size, bias=True)\n",
    "\t\t)\n",
    "        \n",
    "    def forward(self, X: list) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Compute the forward pass through the network.\n",
    "        This is not a prediction, and it should not apply softmax.\n",
    "\n",
    "        Params:\n",
    "            X: the input data\n",
    "\n",
    "        Returns:\n",
    "            The output of the model; i.e. its predictions.\n",
    "        \n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding_layer(X)\n",
    "        flat_embedded = self.flatten(embedded)\n",
    "        logits = self.linear_relu_stack(flat_embedded)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66b6a91-2c8b-4072-815d-fd83b0a6fbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# Defining a training function that goes over every batch per epoch\n",
    "def train_one_epoch(dataloader, model, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        # Separating the input + label pair for each instance\n",
    "        inputs, labels = data\n",
    "        \n",
    "\t\t# Zeroing gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\t\t# Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "\t\t# Compute loss and gradients\n",
    "        batch_loss = loss_fn(outputs, labels)\n",
    "        batch_loss.backward()\n",
    "        \n",
    "\t\t# Adjust learning weights\n",
    "        optimizer.step()\n",
    "        \n",
    "\t\t# Adding to epoch loss\n",
    "        epoch_loss += batch_loss.item() # Covert scalar tensor into floating-point\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "# Defining a general training function that goes over all the epochs\n",
    "def train(dataloader, model, epochs: int = 1, lr: float = 0.001) -> None:\n",
    "    \"\"\"\n",
    "    Our model's training loop.\n",
    "    Print the cross entropy loss every epoch.\n",
    "    You should use the Adam optimizer instead of SGD.\n",
    "\n",
    "    When looking for documentation, try to stay on PyTorch's website.\n",
    "    This might be a good place to start: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html \n",
    "    They should have plenty of tutorials, and we don't want you to get confused from other resources.\n",
    "\n",
    "    Params:\n",
    "        dataloader: The training dataloader\n",
    "        model: The model we wish to train\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: Learning rate \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # you will need to initialize an optimizer and a loss function, which you should do\n",
    "    # before the training loop\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Adam optimizer instead of SGD\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() # Multinomial Cross Entropy Loss that applies log-softmax internally and computes the negative log likelihood\n",
    "    \n",
    "    n_batches = len(dataloader)\n",
    "    \n",
    "\t# Making sure gradient tracking is on before start training\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        epoch_loss = train_one_epoch(dataloader, model, optimizer, loss_fn)\n",
    "        avg_epoch_loss = epoch_loss/n_batches\n",
    "        print(f\"Epoch: {epoch}, Loss: {avg_epoch_loss}\\n\")\n",
    "\n",
    "    # print out the epoch number and the current average loss after each epoch\n",
    "    # you can use tqdm to print out a progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b981e7",
   "metadata": {},
   "source": [
    "For the next part, we're testing our model's functions so we can see if it works.\n",
    "No need to do this on both the word and character data, just one is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0024e019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (embedding_layer): Embedding(25374, 50)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=25374, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create your model\n",
    "# Print out its architecture (use the imported summary function)\n",
    "\n",
    "model = FFNN(vocab_size=word_vocab_size, ngram=NGRAM, embedding_layer=word_embedder)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf2d1d4-c860-46b6-a44e-c1627a2d77f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:30<00:00, 90.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.768263207713861\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# train your models for 1 epoch\n",
    "# see timing information posted on Canvas!\n",
    "\n",
    "# re-create your data loader fresh\n",
    "word_dataloader_train, word_dataloader_test = create_dataloaders(x_word, y_word, NUM_SEQUENCES_PER_BATCH)\n",
    "char_dataloader_train, char_dataloader_test = create_dataloaders(x_char, y_char, NUM_SEQUENCES_PER_BATCH)\n",
    "\n",
    "# train your model\n",
    "train(word_dataloader_train, model, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6755443f",
   "metadata": {},
   "source": [
    "10. You're reporting the loss after each epoch of training. What is the loss for your model after 1 epoch?\n",
    "- word or character-based? __word based__\n",
    "- loss? __5.72__\n",
    "- time __03 m 58 s__\n",
    "\n",
    "Loss isn't accuracy, but it does tell us whether or not the model is improving over time. For character-based, loss after one epoch should be ~2.1; for word-based it is ~5.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa31e2d",
   "metadata": {},
   "source": [
    "### e) create a full pipeline (13 points)\n",
    "\n",
    "We've made all the pieces that you'll need for a full pipeline, now let's package everything together nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b22dfd78-fc75-41eb-ba67-17b1141ad42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 points\n",
    "\n",
    "# make a function that does your full *training* pipeline\n",
    "# This is essentially pulling the pieces that you've done so far earlier in this \n",
    "# notebook into a single function that you can call to train your model\n",
    "\n",
    "\n",
    "def full_pipeline(data, word_embeddings_filename: str, \n",
    "                batch_size:int = NUM_SEQUENCES_PER_BATCH,\n",
    "                ngram:int = NGRAM, hidden_units = 128, epochs = 1,\n",
    "                lr = 0.001, test_pct = 0.1\n",
    "                ) -> FFNN:\n",
    "    \"\"\"\n",
    "    Run the entire pipeline from loading embeddings to training.\n",
    "    You won't use the test set for anything.\n",
    "\n",
    "    Params:\n",
    "        data: The raw data to train on, parsed as a list of lists of tokens\n",
    "        word_embeddings_filename: The filename of the Word2Vec word embeddings\n",
    "        batch_size: The batch size to use\n",
    "        hidden_units: The number of hidden units to use\n",
    "        epochs: The number of epochs to train for\n",
    "        lr: The learning rate to use\n",
    "        test_pct: The proportion of samples to use in the test set.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    # Loading embeddings\n",
    "    token_embeddings = nutils.load_word2vec(word_embeddings_filename)\n",
    "    embedder = nutils.create_embedder(token_embeddings)\n",
    "    \n",
    "\t# Encode tokens\n",
    "    encoded_tokens = encode_tokens(data, embedder)\n",
    "    \n",
    "\t# Define vocab size from embedder\n",
    "    vocab_size = embedder.num_embeddings\n",
    "    \n",
    "\t# Prepare training samples\n",
    "    training_sample = generate_ngram_training_samples(encoded_tokens, ngram)\n",
    "    \n",
    "\t# Split sequences\n",
    "    x_sample, y_sample = split_sequences(training_sample)\n",
    "    \n",
    "\t# Create training dataloader\n",
    "    dataloader_train, _ = create_dataloaders(x_sample, y_sample, batch_size, test_pct)\n",
    "\n",
    "\t# Create FFNN model\n",
    "    model = FFNN(vocab_size=vocab_size, ngram=ngram, embedding_layer=embedder, hidden_units=hidden_units)\n",
    "\n",
    "\t# Train our model\n",
    "    train(dataloader=dataloader_train, model=model, epochs=epochs, lr=lr)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24546581",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TRAIN_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 10 points\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use your full pipeline to train models on the word data and the character data.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Importing data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m char_data \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mread_file_spooky(\u001b[43mTRAIN_FILE\u001b[49m, NGRAM, by_character\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m text_data \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mread_file_spooky(TRAIN_FILE, NGRAM, by_character\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Defining base models with default params\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TRAIN_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "# 10 points\n",
    "\n",
    "# Use your full pipeline to train models on the word data and the character data.\n",
    "# Feel free to add cells if you'd like to.\n",
    "\n",
    "# Train your models however you'd like. Play around with number of epochs, learning rate, etc.\n",
    "# Do whatever you'd like to for exploring hyperparameters.\n",
    "# You aren't required to hit a certain loss, but you should leave code here that shows\n",
    "# that you explored effects of changing at least two of the different hyperparameters\n",
    "# Please don't change the architecture of the model (keep it a 2-layer model with 1 hidden layer)\n",
    "\n",
    "# You'll likely want to do this exploration AFTER completing your prediction and generation code, so start\n",
    "# with just training for 1 - 5 epochs with default params.\n",
    "\n",
    "\n",
    "# Word-based takes Felix's computer 7 - 8 min for 5 epochs with default params running on CPU\n",
    "# Char-based Felix's computer ~1min 30sec - 2min for 5 epochs with default params running on CPU\n",
    "\n",
    "# Importing data\n",
    "char_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "text_data = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)\n",
    "\n",
    "# Defining base models with default params\n",
    "\n",
    "base_word_model = full_pipeline(data=text_data, word_embeddings_filename=EMBEDDING_SAVE_FILE_WORD)\n",
    "base_char_model = full_pipeline(data=char_data, word_embeddings_filename=EMBEDDING_SAVE_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667fd254",
   "metadata": {},
   "source": [
    "With default parameters:\n",
    "* Word <br>\n",
    "Time: 03m 51s <br>\n",
    "Loss: 5.77<br>\n",
    "<br>\n",
    "* Character <br>\n",
    "Time: 02m 33s<br>\n",
    "Loss: 2.08\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1171c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you're happy with them, save both models\n",
    "# Feel free to play around with any hyperparameters you'd like\n",
    "\n",
    "# using torch.save and the model's state_dict\n",
    "torch.save(base_word_model.state_dict(), MODEL_FILE_WORD)\n",
    "torch.save(base_char_model.state_dict(), MODEL_FILE_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6861bf97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_char_wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m FINAL_EMBEDDING_CHAR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings/spooky_embedding_char_100.model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#load the embeddings \u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#final_char_wv = nutils.load_word2vec(FINAL_EMBEDDING_CHAR)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#final_char_embedder = nutils.create_embedder(final_char_wv)\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfinal_char_wv\u001b[49m\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m##final_word_wv = nutils.load_word2vec(FINAL_EMBEDDING_WORD)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m final_word_embedder \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mcreate_embedder(final_word_wv)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'final_char_wv' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyper parameters we decided on after multiple iterations\n",
    "\"\"\"\n",
    "Char-----\n",
    "Epochs: 50\n",
    "Batch Size: 512\n",
    "Hidden Units: 512\n",
    "Embedding size: 100\n",
    "Learning Rate: 0.001\n",
    "Ngram: 5\n",
    "\n",
    "Word-----\n",
    "Epochs: 50\n",
    "Batch Size: 128\n",
    "Hidden Units: 512\n",
    "Embedding size: 100\n",
    "Learning Rate: 0.001\n",
    "Ngram: 5\n",
    "\n",
    "\"\"\"\n",
    "FINAL_EMBEDDING_WORD = f'embeddings/spooky_embedding_word_100.model'\n",
    "FINAL_EMBEDDING_CHAR = f'embeddings/spooky_embedding_char_100.model'\n",
    "\n",
    "#load the embeddings \n",
    "#final_char_wv = nutils.load_word2vec(FINAL_EMBEDDING_CHAR)\n",
    "#final_char_embedder = nutils.create_embedder(final_char_wv)\n",
    "print(final_char_wv.wv.vectors.shape)\n",
    "\n",
    "##final_word_wv = nutils.load_word2vec(FINAL_EMBEDDING_WORD)\n",
    "final_word_embedder = nutils.create_embedder(final_word_wv)\n",
    "\n",
    "\n",
    "print(\"Character Embedding Vocab Size:\", len(final_char_wv.wv))\n",
    "print(\"Word Embedding Vocab Size:\", len(final_word_wv.wv))\n",
    "\n",
    "word_vocab_size = len(final_word_wv.wv)\n",
    "char_vocab_size = len(final_char_wv.wv)\n",
    "\n",
    "FINAL_FILE_WORD = f'spooky_author_model_char_3.pt'\n",
    "\n",
    "#FINAL_FILE_WORD = f'models/spooky_author_model_word_100_512_5_512_50_0.001.pt'\n",
    "FINAL_FILE_CHAR = f'models/spooky_author_model_char_100_128_5_512_50_0.001.pt'\n",
    "\n",
    "print(\"Word Embedding Vocab Size:\", len(final_word_wv.wv))\n",
    "print(\"Char Embedding Vocab Size:\", len(final_char_wv.wv))\n",
    "\n",
    "# load the models in:\n",
    "\n",
    "final_word_model = FFNN(vocab_size=25374, ngram=5, embedding_layer=word_embedder, hidden_units=512)\n",
    "final_char_model = FFNN(vocab_size=25374, ngram=5, embedding_layer=char_embedder, hidden_units=512)\n",
    "\n",
    "#model_dict = final_word_model.state_dict()\n",
    "\n",
    "#if checkpoint[\"embedding_layer.weight\"].shape[0] != model_dict[\"embedding_layer.weight\"].shape[0]:\n",
    "    #print(\"Resizing embedding weights...\")\n",
    "   # checkpoint[\"embedding_layer.weight\"] = checkpoint[\"embedding_layer.weight\"][: model_dict[\"embedding_layer.weight\"].shape[0]]\n",
    "\n",
    "# Update model state dict\n",
    "#model_dict.update(checkpoint)\n",
    "#final_word_model.load_state_dict(model_dict)\n",
    "print(\"Final Word Model Embedding Layer Shape:\", final_word_model.embedding_layer.weight.shape)\n",
    "print(\"Final Char Model Embedding Layer Shape:\", final_char_model.embedding_layer.weight.shape)\n",
    "\n",
    "checkpoint = torch.load(FINAL_FILE_WORD, map_location=torch.device('cpu'))\n",
    "print(\"Checkpoint Embedding Layer Shape:\", checkpoint[\"embedding_layer.weight\"].shape)\n",
    "\n",
    "final_word_model.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "#final_word_model.load_state_dict(torch.load(FINAL_FILE_WORD),map_location=torch.device('cpu'),strict= false)\n",
    "final_char_model.load_state_dict(torch.load(FINAL_FILE_CHAR),map_location=torch.device('cpu'),strict= false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb41ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word_wv = nutils.load_word2vec(FINAL_EMBEDDING_WORD)\n",
    "final_word_embedder = nutils.create_embedder(final_word_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0bc18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word_vocab_size = final_word_embedder.num_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "905d4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_char_data = nutils.read_file_spooky(TRAIN_FILE, ngram=5, by_character=True)\n",
    "final_text_data = nutils.read_file_spooky(TRAIN_FILE, ngram=5, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ded9266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_word_model = FFNN(vocab_size=final_word_vocab_size, ngram=5, embedding_layer=final_word_embedder, hidden_units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f143ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_word_model.load_state_dict(torch.load(\"models/spooky_author_model_word_100_512_5_512_50_0.001.pt\", map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e554fe9-a413-4471-96fe-a51e3764c2ae",
   "metadata": {},
   "source": [
    "### f) Generate Sentences (25 points)\n",
    "\n",
    "Now that you have trained models, you'll work on the generation piece. Note that because you saved your models, even if you have to re-start your kernel, you should be able to re-load them without having to re-train them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad01dd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the models in again with code like:\n",
    "base_word_model = FFNN(vocab_size=word_vocab_size, ngram=NGRAM, embedding_layer=word_embedder, hidden_units=128)\n",
    "base_char_model = FFNN(vocab_size=char_vocab_size, ngram=NGRAM, embedding_layer=char_embedder, hidden_units=128)\n",
    "\n",
    "base_word_model.load_state_dict(torch.load(MODEL_FILE_WORD))\n",
    "base_char_model.load_state_dict(torch.load(MODEL_FILE_CHAR))\n",
    "\n",
    "# then switch the model into evaluation mode\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "608b05fb-8db9-4d44-8cb2-bb9ec5f373d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points \n",
    "\n",
    "# Create a function that predicts the next token in a sequence.\n",
    "def predict(model, input_tokens) -> str:\n",
    "    \"\"\"\n",
    "    Get the model's next word prediction for an input.\n",
    "    This is where you'll use the softmax function!\n",
    "    Assume that the input tokens do not contain any unknown tokens.\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        input_tokens: A list of natural-language tokens. Must be length N-1.\n",
    "\n",
    "    Returns:\n",
    "        The predicted token (not the predicted index!)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "\t# Encode tokens\n",
    "    encoded_tokens = [model.embedding_layer.token_to_index[token] for token in input_tokens]\n",
    "    \n",
    "\t# Trasform to tensor\n",
    "    encoded_tokens = torch.tensor([encoded_tokens]) # Dim [1, ngram-1]\n",
    "    \n",
    "    # Setting model to evaluation mode turns off Dropout and BatchNorm making the predictions deterministic\n",
    "    model.eval()  # Set the model to evaluation mode if you haven't already\n",
    "    \n",
    "    with torch.no_grad(): # Speeds up inference and reduces memory usage by not having to calcualte gradients\n",
    "        logits = model(encoded_tokens) # Forward pass on the model\n",
    "        probability = nn.functional.softmax(logits, dim=1) # Normalize z scores to probability\n",
    "        predicted_idx = torch.multinomial(probability, num_samples=1).item()\n",
    "\n",
    "        #predicted_idx = probability.argmax(dim=1).item() # Retrieve int value\n",
    "\t\t\n",
    "\t# Transform index to natural-language token\n",
    "    predicted_token = model.embedding_layer.index_to_token[predicted_idx] \n",
    "    \n",
    "    return predicted_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f721708d-3dde-4e91-888d-080c4ac6ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "from typing import List\n",
    "# Generate a sequence from the model until you get an end of sentence token.\n",
    "def generate(model, seed: List[str], max_tokens: int = None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Use the trained model to generate a sentence.\n",
    "    This should be somewhat similar to generation for HW2...\n",
    "    Make sure to use your predict function!\n",
    "\n",
    "    Params:\n",
    "        model: Your trained model\n",
    "        seed: [w_1, w_2, ..., w_(n-1)].\n",
    "        max_tokens: The maximum number of tokens to generate. When None, should gener\n",
    "            generate until the end of sentence token is reached.\n",
    "\n",
    "    Return:\n",
    "        A list of generated tokens.\n",
    "    \"\"\" \n",
    "    n_tokens = 0 # Count tokens that have been generated\n",
    "    tokens = seed.copy() # Copy of initial seed\n",
    "    end_token = \"<\\s>\"\n",
    "    \n",
    "    while True:\n",
    "        for_prediction = seed[-(model.ngram-1):]\n",
    "        predicted_token = predict(model, for_prediction)\n",
    "        if predicted_token == end_token:\n",
    "        \tbreak\n",
    "        tokens.append(predicted_token)\n",
    "        n_tokens += 1\n",
    "        if max_tokens is not None and n_tokens >= max_tokens:\n",
    "            break\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7734ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentences(model, seed: List[str],  n_sentences: int, max_tokens: int = None) -> List[str]:\n",
    "    return [generate(model, seed, max_tokens) for i in range(n_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0219cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might want to define some functions to help you format the text nicely\n",
    "# and/or generate multiple sequences\n",
    "\n",
    "def format_sentence(tokens_list: List[List[str]], by_char = False) -> str:\n",
    "  \"\"\"Removes <s> at the start of the sentence and </s> at ehe end. Joins the list of tokens into a string and capitalizes it.\n",
    "  Args:\n",
    "    tokens (list(list)): the list of tokens list to be formatted into a sentence\n",
    "\n",
    "  Returns:\n",
    "    string: formatted sentence as a string\n",
    "  \n",
    "  \"\"\"\n",
    "  text = \"\" # Initializing final sentence\n",
    "  for tokens in tokens_list: # Parsing through each individual sentence\n",
    "    while tokens[0] == '<s>': # Removes all <s> at the beggining even if there are several for ngram > 2 models\n",
    "      tokens.pop(0)\n",
    "    if tokens[-1] == '</s>': # Removes the one </s> at the end of the sentence\n",
    "      tokens.pop(-1)\n",
    "    if by_char:\n",
    "      sentence = \"\".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    else:\n",
    "      sentence = \" \".join(tokens) # Converts list of tokens into a string\n",
    "      sentence = sentence.capitalize() # Capitalizes the first letter of each sentence\n",
    "    text += sentence + \".\\n\" # Adds a period and space separator between sentences\n",
    "  return text.strip(\" \") # Removes the last space in the last sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b317a-e8c7-41d4-a598-34318ffda93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'genius', 'floor', 'time', 'about', 'had', 'relented', 'fancied', 'early', 'very', 'melancholy'], ['<s>', 'this', 'chance', 'afforded', 'was', 'jaws', 'robin', 'rarely', 'cohort', 'few', 'wont', 'is'], ['<s>', 'this', 'warm', 'manner', 'leetle', 'is', 'one', 'say', 'young', 'silence', 'time', 'engaging'], ['<s>', 'this', 'thick', ',', 'lucid', 'appeared', 'was', 'time', ',', 'form', 'old', 'closely'], ['<s>', 'this', 'you', 'somewhat', 'sudden', 'i', 'last', 'above', 'complete', 'indignation', ',', 'in'], ['<s>', 'this', 'was', 'fallacious', 'was', 'of', 'square', 'morning', 'of', 'for', 'crushed', 'little'], ['<s>', 'this', 'event', 'subject', 'came', 'head', 'was', 'few', 'to', 'occupied', 'assured', 'for'], ['<s>', 'this', 'place', 'i', 'will', '.', 'day', 'sirocco', 'words', 'wild', 'pitch', 'can'], ['<s>', 'this', 'stage', 'spot', 'articulate', 'same', 'sort', 'was', 'from', 'i', 'vanish', 'conduct'], ['<s>', 'this', 'body', 'capable', 'when', ',', 'strange', 'right', ',', 'if', 'were', 'cold']]\n",
      "This genius floor time about had relented fancied early very melancholy.\n",
      "This chance afforded was jaws robin rarely cohort few wont is.\n",
      "This warm manner leetle is one say young silence time engaging.\n",
      "This thick , lucid appeared was time , form old closely.\n",
      "This you somewhat sudden i last above complete indignation , in.\n",
      "This was fallacious was of square morning of for crushed little.\n",
      "This event subject came head was few to occupied assured for.\n",
      "This place i will . day sirocco words wild pitch can.\n",
      "This stage spot articulate same sort was from i vanish conduct.\n",
      "This body capable when , strange right , if were cold.\n",
      "\n",
      "Tehhhhhhhhh.\n",
      "Thohhhhhhhh.\n",
      "Thohhhohhhh.\n",
      "Thhohhhhhah.\n",
      "Thhhhhhehoh.\n",
      "Tohhhhoihha.\n",
      "Thhhhhhoehh.\n",
      "Thhhhohhohh.\n",
      "Thhohhehhho.\n",
      "Thhhohihhhh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# generate and display ten sequences from both your word model and your character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# For character-based, replace _ with a space\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "word_test = [\"<s>\", \"this\"]\n",
    "char_test = [\"<s>\", \"t\"]\n",
    "\n",
    "word_generated = generate_sentences(model=base_word_model, seed=word_test, n_sentences=10, max_tokens=10)\n",
    "char_generated = generate_sentences(model=base_char_model, seed=char_test, n_sentences=10, max_tokens=10)\n",
    "\n",
    "print(word_generated)\n",
    "print(format_sentence(word_generated))\n",
    "print(format_sentence(char_generated, by_char=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0885a20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This noble , marked claim view spot revelation motion change friend.\\nThis essay room is path light appearance fusion apparatus extraordinary peculiarity.\\nThis building panorama is may circumstance latter task of was aroused.\\nThis has is was is house i is reply expectation period.\\nThis condition latter or year machinery seemed reason prison idea elevation.\\nThis same spot difference is small was plainly might done was.\\nThis thing again point year spirit old expectation lakelet spirit let.\\nThis occasion attack claim revelation generous matter is was path effect.\\nThis apparatus prolongation prolongation occurred could latter behavior grief twenty question.\\nThis machinery event latter poor thirst result prolongation dwarf small seemed.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_sentence(generate_sentences(final_word_model, [\"<s>\", \"<s>\", \"<s>\", \"<s>\", \"this\"], 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a1ec4-9a8b-46dc-894d-b41be323058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Generate 100 example sentences with each model and save them to two files, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model\n",
    "# We've defined the filenames for you at the top of this notebook\n",
    "# Do not print these sentences here :)\n",
    "\n",
    "word_generated_final = generate_sentences(model=base_word_model, seed=word_test, n_sentences=100, max_tokens=10)\n",
    "char_generated_final = generate_sentences(model=base_char_model, seed=char_test, n_sentences=100, max_tokens=10)\n",
    "\n",
    "torch.save(format_sentence(word_generated_final), OUTPUT_WORDS)\n",
    "torch.save(format_sentence(char_generated_final, by_char=True), OUTPUT_CHARS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caef7860",
   "metadata": {},
   "source": [
    "11. What were the final parameters that you used for your model? \n",
    "- N: __3__\n",
    "- embedding size: __50__\n",
    "- epochs: __5__\n",
    "- hidden units: __128__\n",
    "- learning rate: __0.001__\n",
    "- training time + system you were running it on (operating system + chip/specs): __Training time: ~11min\n",
    "System: macOS 15.2, M1 chip 2020__\n",
    "    - for pairs, you can either note both partners' training times or just one\n",
    "\n",
    "- What was the word-based model's final loss? __4.676736436582088__\n",
    "- Character based? __1.9567836109068009__\n",
    "\n",
    "If you used different parameters for your word-based and character-based models, note the different parameters clearly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
