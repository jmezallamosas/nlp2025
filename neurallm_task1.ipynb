{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5313d72c-9835-45fb-bfcb-90cf977062b6",
   "metadata": {},
   "source": [
    "Homework 4: Neural Language Models (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data)\n",
    "----\n",
    "\n",
    "Due date: March 13th, 2025 @ 9pm Boston time\n",
    "\n",
    "Points: (will be listed on Canvas)\n",
    "\n",
    "Goals:\n",
    "- explore & use word embeddings\n",
    "- train neural models from the ground up!\n",
    "- get comfy with a modern neural net library (`pytorch`)\n",
    "    - you'll likelye make close friends with the docs: https://pytorch.org/tutorials/beginner/basics/intro.html \n",
    "- evaluate neural vs. vanilla n-gram language models\n",
    "\n",
    "Complete in groups of: __two (pairs)__. If you prefer to work on your own, you may, but be aware that this homework has been designed as a partner project!\n",
    "\n",
    "Allowed python modules:\n",
    "- `gensim`, `numpy`, `matplotlib`, `pytorch`, `nltk`, `pandas`, `sci-kit learn` (`sklearn`), `seaborn`, all built-in python libraries (e.g. `math` and `string`), and anything else we imported in the starter code\n",
    "- if you would like to use a library not on this list, post on piazza to request permission\n",
    "- all *necessary* imports have been included for you (all imports that we used in our solution)\n",
    "\n",
    "Instructions:\n",
    "- Complete outlined problems in this notebook. \n",
    "- When you have finished, __clear the kernel__ and __run__ your notebook \"fresh\" from top to bottom. Ensure that there are __no errors__. \n",
    "    - If a problem asks for you to write code that does result in an error (as in, the answer to the problem is an error), leave the code in your notebook but commented out so that running from top to bottom does not result in any errors.\n",
    "- Double check that you have completed Task 0.\n",
    "- Submit your work on Gradescope.\n",
    "- Double check that your submission on Gradescope looks like you believe it should __and__ that all partners are included (for partner work).\n",
    "\n",
    "Data processing:\n",
    "- You may __choose__ how you would like to tokenize your text for this assignment\n",
    "- You'll want to __deal with internal commas (commas inside of the sentences) appropriately__ when you read in the data, so use the python [`csv` module](https://docs.python.org/3/library/csv.html) or some other module to read the csv in (vs. splitting on commas).\n",
    "\n",
    "Warnings:\n",
    "- You might see:\n",
    "```\n",
    "notebook controller is DISPOSED. \n",
    "View Jupyter log for further details.\n",
    "```\n",
    "This is not an error per se--go to the last cell that ran successfully (or the first cell) and run them one-by-one, waiting for the prior one to finish running before moving to the next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd6bfa",
   "metadata": {},
   "source": [
    "Names\n",
    "----\n",
    "Names:<br>\n",
    "__Katherine Aristizabal Norena__<br>\n",
    "__Jose Meza Llamosas__<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c2d46e",
   "metadata": {},
   "source": [
    "Task 1: Provided Data Write-Up (7 points)\n",
    "---\n",
    "\n",
    "Every time you use a data set in an NLP application (or in any software application), you should be able to answer a set of questions about that data. Answer these now. Default to no more than 1 sentence per question needed. If more explanation is necessary, do give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d73016",
   "metadata": {},
   "source": [
    "This is about the __provided__ ðŸŽƒ spooky ðŸ‘» authors ðŸ§Ÿ data set. Please __bold__ your answers to all written questions! Each row in this dataset represents one sentence.\n",
    "\n",
    "1. Where did you get the data from? The provided dataset is the training data from: https://www.kaggle.com/competitions/spooky-author-identification \n",
    "2. (1 pt) How was the data collected (where did the people acquiring the data get it from and how)? \n",
    "__The dataset was collected from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenize.__\n",
    "3. (1 pt) What is your data? (i.e. newswire, tweets, books, blogs, etc)\n",
    "__Text extracts from books written by spooky authors__\n",
    "4. (1 pt) Who produced the data? (who were the authors of the text? Your answer might be a specific person or a particular group of people) __The author of the collection is Kaggle itself, but the authors books from which the text extracts were collected are Edgar Allan Poe, HP Lovecraft and Mary Shelley__\n",
    "5. (1 pt) How large is the dataset? (# texts/sentences, # total tokens by word) __19579 total text/sentences, and 634080 total tokens by word__\n",
    "6. (1 pt) What are the minimum, maximum, and average sentence lengths (by tokens) in your dataset? __Maximum length is 878 tokens, minimum length is 6 tokens, and the average is 32.39 tokens__\n",
    "7. (2 pts) How large is the vocabulary, both tokenized by character and by word? __The vocabulary size for tokenized by words is 25374 tokens, and for tokenized by characters is 60 tokens.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import your libraries here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Remember to restart your kernel if you change the contents of this file!\n",
    "import neurallm_utils as nutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9b63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that you need to answer the above questions here!\n",
    "# but make sure that you leave the answers you want us to grade in the markdown!\n",
    "\n",
    "# Loading dataset\n",
    "TRAIN_FILE = \"spooky_author_train.csv\"\n",
    "data = pd.read_csv(TRAIN_FILE)\n",
    "tokens_word = nutils.read_file_spooky(TRAIN_FILE, by_character = False, ngram=1)\n",
    "tokens_char = nutils.read_file_spooky(TRAIN_FILE, by_character= True, ngram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f28a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts/sentences: 19579\n",
      "Number of tokens: 634080\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of texts/sentences: {len(data)}\")\n",
    "print(f\"Number of tokens: {len([item for sublist in tokens_word for item in sublist])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e024c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tokens = []\n",
    "for sentence in tokens_word:\n",
    "    count_tokens.append(len(sentence))\n",
    "max_len = np.max(count_tokens)\n",
    "min_len = np.min(count_tokens)\n",
    "mean_len = np.mean(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791972cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max length by tokens is:  878.00\n",
      "The min length by tokens is:  6.00\n",
      "The average length by tokens is:  32.39\n"
     ]
    }
   ],
   "source": [
    "print(f\"The max length by tokens is: {max_len: .2f}\")\n",
    "print(f\"The min length by tokens is: {min_len: .2f}\")\n",
    "print(f\"The average length by tokens is: {mean_len: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f56bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_word_total = set([item for sublist in tokens_word for item in sublist])\n",
    "tokens_char_total = set([item for sublist in tokens_char for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c265d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for tokenized by word is 25374 tokens and for tokenized by character is 60 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"The vocabulary size for tokenized by word is {len(tokens_word_total)} tokens and for tokenized by character is {len(tokens_char_total)} tokens\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
